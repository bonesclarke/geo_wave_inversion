{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":39763,"databundleVersionId":11756775,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport yaml\nimport time\nimport csv\nimport random\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold, train_test_split\nimport seaborn as sns\n\nimport scipy.stats as stats\nimport scipy.signal as signal\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torchsummary import summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:21:05.624946Z","iopub.execute_input":"2025-05-12T16:21:05.625518Z","iopub.status.idle":"2025-05-12T16:21:05.630982Z","shell.execute_reply.started":"2025-05-12T16:21:05.625494Z","shell.execute_reply":"2025-05-12T16:21:05.630082Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Set random seeds for reproducibility\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:21:05.632262Z","iopub.execute_input":"2025-05-12T16:21:05.632750Z","iopub.status.idle":"2025-05-12T16:21:05.646095Z","shell.execute_reply.started":"2025-05-12T16:21:05.632731Z","shell.execute_reply":"2025-05-12T16:21:05.645321Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Helper functions for data loading\ndef map_seismic_to_velocity_path(input_file):\n    \"\"\"Convert seismic data path to velocity model path\"\"\"\n    return Path(str(input_file).replace('seis', 'vel').replace('data', 'model'))\n\ndef get_train_files(data_path):\n    \"\"\"Find all seismic data files and map to velocity model files\"\"\"\n    # Find all seismic data files (containing 'seis' or 'data' in filename)\n    input_files = [\n        f for f in Path(data_path).rglob('*.npy')\n        if ('seis' in f.stem) or ('data' in f.stem)\n    ]\n    \n    # Map each input file to its corresponding output file\n    output_files = [map_seismic_to_velocity_path(f) for f in input_files]\n    \n    # Verify all output files exist\n    missing_files = [f for f in output_files if not f.exists()]\n    if missing_files:\n        raise FileNotFoundError(f\"Missing velocity model files: {missing_files[:5]}...\")\n    \n    return input_files, output_files","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:21:05.647581Z","iopub.execute_input":"2025-05-12T16:21:05.647843Z","iopub.status.idle":"2025-05-12T16:21:05.653174Z","shell.execute_reply.started":"2025-05-12T16:21:05.647820Z","shell.execute_reply":"2025-05-12T16:21:05.652512Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"# Data Load for Analysis","metadata":{}},{"cell_type":"code","source":"# Data analysis functions\ndef analyze_dataset_statistics(input_files, output_files, n_samples=10):\n    \"\"\"Analyze basic statistics of the dataset\"\"\"\n    input_stats = []\n    output_stats = []\n    \n    # Sample files for analysis\n    sample_indices = random.sample(range(len(input_files)), min(n_samples, len(input_files)))\n    \n    for idx in tqdm(sample_indices, desc=\"Analyzing files\"):\n        input_data = np.load(input_files[idx])\n        output_data = np.load(output_files[idx])\n        \n        # Print shapes to understand data structure\n        print(f\"Input data shape: {input_data.shape}\")\n        print(f\"Output data shape: {output_data.shape}\")\n        \n        # Collect statistics for input data\n        input_stats.append({\n            'file': input_files[idx].name,\n            'shape': input_data.shape,\n            'min': np.min(input_data),\n            'max': np.max(input_data),\n            'mean': np.mean(input_data),\n            'median': np.median(input_data),\n            'std': np.std(input_data),\n            'skew': stats.skew(input_data.reshape(-1)),\n            'kurtosis': stats.kurtosis(input_data.reshape(-1)),\n            'zeros_pct': np.mean(input_data == 0) * 100,\n            'unique_values': len(np.unique(input_data)),\n        })\n        \n        # Collect statistics for output data\n        output_stats.append({\n            'file': output_files[idx].name,\n            'shape': output_data.shape,\n            'min': np.min(output_data),\n            'max': np.max(output_data),\n            'mean': np.mean(output_data),\n            'median': np.median(output_data),\n            'std': np.std(output_data),\n            'skew': stats.skew(output_data.reshape(-1)),\n            'kurtosis': stats.kurtosis(output_data.reshape(-1)),\n            'zeros_pct': np.mean(output_data == 0) * 100,\n            'unique_values': len(np.unique(output_data)),\n        })\n    \n    input_df = pd.DataFrame(input_stats)\n    output_df = pd.DataFrame(output_stats)\n    \n    print(\"\\n=== INPUT DATA STATISTICS ===\")\n    print(input_df[['min', 'max', 'mean', 'median', 'std', 'zeros_pct']].describe())\n    \n    print(\"\\n=== OUTPUT DATA STATISTICS ===\")\n    print(output_df[['min', 'max', 'mean', 'median', 'std', 'zeros_pct']].describe())\n    \n    return input_df, output_df, input_data.shape, output_data.shape\n\ndef visualize_sample_pair(input_file, output_file, index=0, sample_idx=0, channel_idx=0, figsize=(15, 10)):\n    \"\"\"Visualize a sample pair of input seismic data and output velocity model\"\"\"\n    input_data = np.load(input_file)\n    output_data = np.load(output_file)  # Fixed: using the passed parameter\n    \n    print(f\"Input data shape: {input_data.shape}\")\n    print(f\"Output data shape: {output_data.shape}\")\n    \n    # Based on your data structure: [samples, channels, height, width]\n    # Select a specific sample and channel\n    seismic_sample = input_data[sample_idx, channel_idx]\n    velocity_sample = output_data[sample_idx, 0]  # Assuming output has 1 channel\n    \n    print(f\"Seismic sample shape: {seismic_sample.shape}\")\n    print(f\"Velocity sample shape: {velocity_sample.shape}\")\n    \n    # Create figure\n    fig, axs = plt.subplots(2, 2, figsize=figsize)\n    \n    # Plot seismic data\n    im0 = axs[0, 0].imshow(seismic_sample, aspect='auto', cmap='seismic')\n    axs[0, 0].set_title(f'Seismic Data - Sample {sample_idx}, Channel {channel_idx}')\n    axs[0, 0].set_xlabel('X position')\n    axs[0, 0].set_ylabel('Time/Depth')\n    plt.colorbar(im0, ax=axs[0, 0], fraction=0.046, pad=0.04)\n    \n    # Plot velocity model\n    im1 = axs[0, 1].imshow(velocity_sample, aspect='auto', cmap='viridis')\n    axs[0, 1].set_title(f'Velocity Model - Sample {sample_idx}')\n    axs[0, 1].set_xlabel('X position')\n    axs[0, 1].set_ylabel('Depth')\n    plt.colorbar(im1, ax=axs[0, 1], fraction=0.046, pad=0.04)\n    \n    # Plot histograms\n    axs[1, 0].hist(seismic_sample.ravel(), bins=50, alpha=0.7, color='blue')\n    axs[1, 0].set_title('Seismic Data Histogram')\n    axs[1, 0].set_xlabel('Amplitude')\n    axs[1, 0].set_ylabel('Frequency')\n    \n    axs[1, 1].hist(velocity_sample.ravel(), bins=50, alpha=0.7, color='green')\n    axs[1, 1].set_title('Velocity Model Histogram')\n    axs[1, 1].set_xlabel('Velocity (m/s)')\n    axs[1, 1].set_ylabel('Frequency')\n    \n    plt.tight_layout()\n    plt.savefig(f'/kaggle/working/{sample_idx}_visualize_sample_pair_channel{channel_idx}.png')\n    plt.show()\n    \n    return seismic_sample, velocity_sample\n\ndef visualize_channels(input_file, sample_idx=0, figsize=(20, 15)):\n    \"\"\"Visualize all channels of a seismic data sample\"\"\"\n    input_data = np.load(input_file)\n    \n    # Select a specific sample\n    if len(input_data.shape) == 4:  # [samples, channels, height, width]\n        sample_data = input_data[sample_idx]\n        n_channels = sample_data.shape[0]\n    else:\n        raise ValueError(f\"Unexpected data shape: {input_data.shape}\")\n    \n    # Create a grid of subplots - 2 rows with 3 columns should be enough for 5 channels\n    fig, axs = plt.subplots(2, 3, figsize=figsize)\n    \n    # Flatten the axes array for easier indexing\n    axs = axs.flatten()\n    \n    # Plot each channel\n    for i in range(n_channels):\n        channel_data = sample_data[i]\n        \n        im = axs[i].imshow(channel_data, aspect='auto', cmap='seismic')\n        axs[i].set_title(f'Channel {i} - Shape: {channel_data.shape}')\n        axs[i].set_xlabel('X position')\n        axs[i].set_ylabel('Time/Depth')\n        plt.colorbar(im, ax=axs[i], fraction=0.046, pad=0.04)\n    \n    # Hide any unused subplots\n    for i in range(n_channels, len(axs)):\n        axs[i].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig(f'/kaggle/working/{sample_idx}_visualize_channels.png')\n    plt.show()\n    \n    return sample_data\n\ndef analyze_spectral_content(input_file, output_file, sample_idx=0, channel_idx=0, figsize=(15, 10)):\n    \"\"\"Analyze the spectral content of input and output data\"\"\"\n    input_data = np.load(input_file)\n    output_data = np.load(output_file)\n    \n    # Select data based on known shape\n    seismic_sample = input_data[sample_idx, channel_idx]\n    velocity_sample = output_data[sample_idx, 0]  # Assuming output has 1 channel\n    \n    # Compute 2D FFT\n    seismic_fft = np.fft.fft2(seismic_sample)\n    seismic_fft_shifted = np.fft.fftshift(seismic_fft)\n    seismic_magnitude = np.log1p(np.abs(seismic_fft_shifted))\n    \n    velocity_fft = np.fft.fft2(velocity_sample)\n    velocity_fft_shifted = np.fft.fftshift(velocity_fft)\n    velocity_magnitude = np.log1p(np.abs(velocity_fft_shifted))\n    \n    # Create figure\n    fig, axs = plt.subplots(2, 2, figsize=figsize)\n    \n    # Plot original data\n    im0 = axs[0, 0].imshow(seismic_sample, aspect='auto', cmap='seismic')\n    axs[0, 0].set_title(f'Seismic Data - Sample {sample_idx}, Channel {channel_idx}')\n    plt.colorbar(im0, ax=axs[0, 0], fraction=0.046, pad=0.04)\n    \n    im1 = axs[0, 1].imshow(velocity_sample, aspect='auto', cmap='viridis')\n    axs[0, 1].set_title(f'Velocity Model - Sample {sample_idx}')\n    plt.colorbar(im1, ax=axs[0, 1], fraction=0.046, pad=0.04)\n    \n    # Plot FFT magnitude\n    im2 = axs[1, 0].imshow(seismic_magnitude, aspect='auto', cmap='inferno')\n    axs[1, 0].set_title('Seismic Data FFT Magnitude (log scale)')\n    plt.colorbar(im2, ax=axs[1, 0], fraction=0.046, pad=0.04)\n    \n    im3 = axs[1, 1].imshow(velocity_magnitude, aspect='auto', cmap='inferno')\n    axs[1, 1].set_title('Velocity Model FFT Magnitude (log scale)')\n    plt.colorbar(im3, ax=axs[1, 1], fraction=0.046, pad=0.04)\n    \n    plt.tight_layout()\n    plt.savefig(f'/kaggle/working/{sample_idx}_spectral_content_channel{channel_idx}.png')\n    plt.show()\n    \n    return seismic_magnitude, velocity_magnitude\n\ndef analyze_gradient_patterns(input_file, output_file, sample_idx=0, channel_idx=0, figsize=(15, 15)):\n    \"\"\"\n    Analyze gradient patterns in the data\n    \"\"\"\n    input_data = np.load(input_file)\n    output_data = np.load(output_file)\n    \n    # Select data based on known shape\n    seismic_sample = input_data[sample_idx, channel_idx]  # Shape: [1000, 70]\n    velocity_sample = output_data[sample_idx, 0]         # Shape: [70, 70]\n    \n    print(f\"Seismic sample shape: {seismic_sample.shape}\")\n    print(f\"Velocity sample shape: {velocity_sample.shape}\")\n    \n    # Compute gradients\n    seismic_grad_y, seismic_grad_x = np.gradient(seismic_sample)\n    velocity_grad_y, velocity_grad_x = np.gradient(velocity_sample)\n    \n    # Compute gradient magnitude\n    seismic_grad_mag = np.sqrt(seismic_grad_x**2 + seismic_grad_y**2)\n    velocity_grad_mag = np.sqrt(velocity_grad_x**2 + velocity_grad_y**2)\n    \n    # Create figure\n    fig, axs = plt.subplots(3, 3, figsize=figsize)\n    \n    # Plot original data\n    im0 = axs[0, 0].imshow(seismic_sample, aspect='auto', cmap='seismic')\n    axs[0, 0].set_title(f'Seismic Data - Sample {sample_idx}, Channel {channel_idx}')\n    plt.colorbar(im0, ax=axs[0, 0], fraction=0.046, pad=0.04)\n    \n    im1 = axs[0, 1].imshow(velocity_sample, aspect='auto', cmap='viridis')\n    axs[0, 1].set_title(f'Velocity Model - Sample {sample_idx}')\n    plt.colorbar(im1, ax=axs[0, 1], fraction=0.046, pad=0.04)\n    \n    # Plot x-gradients\n    im2 = axs[1, 0].imshow(seismic_grad_x, aspect='auto', cmap='coolwarm')\n    axs[1, 0].set_title('Seismic X-Gradient')\n    plt.colorbar(im2, ax=axs[1, 0], fraction=0.046, pad=0.04)\n    \n    im3 = axs[1, 1].imshow(velocity_grad_x, aspect='auto', cmap='coolwarm')\n    axs[1, 1].set_title('Velocity X-Gradient')\n    plt.colorbar(im3, ax=axs[1, 1], fraction=0.046, pad=0.04)\n    \n    # Instead of scatter plots (which require same sizes), show histogram of gradients\n    axs[1, 2].hist(seismic_grad_x.flatten(), bins=50, alpha=0.5, label='Seismic')\n    axs[1, 2].hist(velocity_grad_x.flatten(), bins=50, alpha=0.5, label='Velocity')\n    axs[1, 2].set_title('X-Gradient Histograms')\n    axs[1, 2].set_xlabel('Gradient Value')\n    axs[1, 2].set_ylabel('Frequency')\n    axs[1, 2].legend()\n    \n    # Plot y-gradients\n    im4 = axs[2, 0].imshow(seismic_grad_y, aspect='auto', cmap='coolwarm')\n    axs[2, 0].set_title('Seismic Y-Gradient')\n    plt.colorbar(im4, ax=axs[2, 0], fraction=0.046, pad=0.04)\n    \n    im5 = axs[2, 1].imshow(velocity_grad_y, aspect='auto', cmap='coolwarm')\n    axs[2, 1].set_title('Velocity Y-Gradient')\n    plt.colorbar(im5, ax=axs[2, 1], fraction=0.046, pad=0.04)\n    \n    # Y-gradient histograms instead of scatter\n    axs[2, 2].hist(seismic_grad_y.flatten(), bins=50, alpha=0.5, label='Seismic')\n    axs[2, 2].hist(velocity_grad_y.flatten(), bins=50, alpha=0.5, label='Velocity')\n    axs[2, 2].set_title('Y-Gradient Histograms')\n    axs[2, 2].set_xlabel('Gradient Value')\n    axs[2, 2].set_ylabel('Frequency')\n    axs[2, 2].legend()\n    \n    # Plot gradient magnitude\n    im6 = axs[0, 2].imshow(np.log1p(seismic_grad_mag), aspect='auto', cmap='inferno')\n    axs[0, 2].set_title('Seismic Gradient Magnitude (log scale)')\n    plt.colorbar(im6, ax=axs[0, 2], fraction=0.046, pad=0.04)\n    \n    plt.tight_layout()\n    plt.savefig(f'/kaggle/working/{sample_idx}_gradient_patterns_channel{channel_idx}.png')\n    plt.show()\n    \n    # return mean gradient values\n    return np.mean(seismic_grad_mag), np.mean(velocity_grad_mag)\n\ndef analyze_initial_pool_effect(input_file, sample_idx=0, channel_idx=0):\n    \"\"\"Analyze the effect of the initial pooling layer in the UNet model\"\"\"\n    input_data = np.load(input_file)\n    \n    # Select a specific sample and channel\n    seismic_sample = input_data[sample_idx, channel_idx]\n    \n    # Convert to torch tensor for processing (add batch and channel dimensions)\n    seismic_tensor = torch.from_numpy(seismic_sample).float().unsqueeze(0).unsqueeze(0)\n    \n    # Apply the initial_pool operation\n    pool = torch.nn.AvgPool2d(kernel_size=(14, 1), stride=(14, 1))\n    pooled_tensor = pool(seismic_tensor)\n    \n    # Convert back to numpy for visualization\n    seismic_np = seismic_sample\n    pooled_np = pooled_tensor.squeeze().numpy()\n    \n    # Create figure\n    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Plot original data\n    im0 = axs[0].imshow(seismic_np, aspect='auto', cmap='seismic')\n    axs[0].set_title(f'Original Seismic Data - Sample {sample_idx}, Channel {channel_idx}')\n    axs[0].set_xlabel('X position')\n    axs[0].set_ylabel('Time/Depth')\n    plt.colorbar(im0, ax=axs[0], fraction=0.046, pad=0.04)\n    \n    # Plot pooled data\n    im1 = axs[1].imshow(pooled_np, aspect='auto', cmap='seismic')\n    axs[1].set_title(f'After AvgPool2d(14,1) - Shape: {pooled_np.shape}')\n    axs[1].set_xlabel('X position')\n    axs[1].set_ylabel('Time/Depth (reduced)')\n    plt.colorbar(im1, ax=axs[1], fraction=0.046, pad=0.04)\n    \n    plt.tight_layout()\n    plt.savefig(f'/kaggle/working/{sample_idx}_initial_pool_effect_channel{channel_idx}.png')\n    plt.show()\n    \n    return seismic_np, pooled_np\n\ndef analyze_frequency_components(input_file, output_file, sample_idx=0, channel_idx=0, figsize=(15, 10)):\n    \"\"\"Analyze and compare frequency components between input and output data\"\"\"\n    input_data = np.load(input_file)\n    output_data = np.load(output_file)\n    \n    # Select data based on known shape\n    seismic_sample = input_data[sample_idx, channel_idx]\n    velocity_sample = output_data[sample_idx, 0]  # Assuming output has 1 channel\n    \n    # Compute frequency content\n    # For seismic data - average across spatial dimension\n    seismic_fft_rows = np.fft.rfft(seismic_sample, axis=0)\n    seismic_fft_cols = np.fft.rfft(seismic_sample, axis=1)\n    \n    seismic_power_rows = np.mean(np.abs(seismic_fft_rows)**2, axis=1)\n    seismic_power_cols = np.mean(np.abs(seismic_fft_cols)**2, axis=0)\n    \n    # For velocity data\n    velocity_fft_rows = np.fft.rfft(velocity_sample, axis=0)\n    velocity_fft_cols = np.fft.rfft(velocity_sample, axis=1)\n    \n    velocity_power_rows = np.mean(np.abs(velocity_fft_rows)**2, axis=1)\n    velocity_power_cols = np.mean(np.abs(velocity_fft_cols)**2, axis=0)\n    \n    # Create figure\n    fig, axs = plt.subplots(2, 2, figsize=figsize)\n    \n    # Plot frequency power spectrum - rows (time/depth axis)\n    freqs_rows = np.fft.rfftfreq(seismic_sample.shape[0])\n    axs[0, 0].semilogy(freqs_rows, seismic_power_rows, label='Seismic')\n    axs[0, 0].semilogy(np.fft.rfftfreq(velocity_sample.shape[0]), velocity_power_rows, label='Velocity')\n    axs[0, 0].set_title('Power Spectrum - Depth/Time Direction')\n    axs[0, 0].set_xlabel('Frequency')\n    axs[0, 0].set_ylabel('Power (log scale)')\n    axs[0, 0].legend()\n    axs[0, 0].grid(True)\n    \n    # Plot frequency power spectrum - columns (spatial axis)\n    freqs_cols = np.fft.rfftfreq(seismic_sample.shape[1])\n    axs[0, 1].semilogy(freqs_cols, seismic_power_cols, label='Seismic')\n    axs[0, 1].semilogy(np.fft.rfftfreq(velocity_sample.shape[1]), velocity_power_cols, label='Velocity')\n    axs[0, 1].set_title('Power Spectrum - Spatial Direction')\n    axs[0, 1].set_xlabel('Frequency')\n    axs[0, 1].set_ylabel('Power (log scale)')\n    axs[0, 1].legend()\n    axs[0, 1].grid(True)\n    \n    # Plot original data for reference\n    im0 = axs[1, 0].imshow(seismic_sample, aspect='auto', cmap='seismic')\n    axs[1, 0].set_title(f'Seismic Data - Sample {sample_idx}, Channel {channel_idx}')\n    plt.colorbar(im0, ax=axs[1, 0], fraction=0.046, pad=0.04)\n    \n    im1 = axs[1, 1].imshow(velocity_sample, aspect='auto', cmap='viridis')\n    axs[1, 1].set_title(f'Velocity Model - Sample {sample_idx}')\n    plt.colorbar(im1, ax=axs[1, 1], fraction=0.046, pad=0.04)\n    \n    plt.tight_layout()\n    plt.savefig(f'/kaggle/working/{sample_idx}_frequency_components_channel{channel_idx}.png')\n    plt.show()\n    \n    # Return frequency data for further analysis\n    return {\n        'seismic_power_rows': seismic_power_rows,\n        'seismic_power_cols': seismic_power_cols,\n        'velocity_power_rows': velocity_power_rows,\n        'velocity_power_cols': velocity_power_cols,\n        'freqs_rows': freqs_rows,\n        'freqs_cols': freqs_cols\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:21:05.682164Z","iopub.execute_input":"2025-05-12T16:21:05.682742Z","iopub.status.idle":"2025-05-12T16:21:05.725372Z","shell.execute_reply.started":"2025-05-12T16:21:05.682718Z","shell.execute_reply":"2025-05-12T16:21:05.724609Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Main analysis function\ndef run_data_analysis(data_path, n_samples=5):\n    \"\"\"Run comprehensive data analysis\"\"\"\n    print(f\"Running data analysis on path: {data_path}\")\n    \n    # Get train files\n    input_files, output_files = get_train_files(data_path)\n    print(f\"Found {len(input_files)} input files and {len(output_files)} output files\")\n    \n    # Basic dataset statistics\n    print(\"\\n=== DATASET STATISTICS ===\")\n    input_stats_df, output_stats_df, input_shape, output_shape = analyze_dataset_statistics(\n        input_files, output_files, n_samples=min(n_samples, len(input_files)))\n    \n    if len(input_files) > 0:\n        # We now know the data shape from analyze_dataset_statistics\n        print(f\"\\nConfirmed input shape: {input_shape}\")\n        print(f\"Confirmed output shape: {output_shape}\")\n        \n        # Based on the actual structure, we have a 4D tensor:\n        # [samples, channels, height, width] = [500, 5, 1000, 70]\n        \n        # Visualize the channels from first sample\n        print(\"\\n=== VISUALIZING CHANNELS ===\")\n        sample_idx = 2 \n        sample_data = visualize_channels(input_files[0], sample_idx=sample_idx)\n        \n        # Visualize each channel\n        for channel_idx in range(min(5, input_shape[1])):  # Look at all 5 channels\n            print(f\"\\n=== SAMPLE VISUALIZATION - CHANNEL {channel_idx} ===\")\n            seismic_sample, velocity_sample = visualize_sample_pair(\n                input_files[0], output_files[0], sample_idx=sample_idx, channel_idx=channel_idx)\n            \n            # Set channel\n            #if channel_idx == 0:\n            print(\"\\n=== SPECTRAL ANALYSIS ===\")\n            analyze_spectral_content(input_files[0], output_files[0], \n                                    sample_idx=sample_idx, channel_idx=channel_idx)\n            \n            print(\"\\n=== GRADIENT PATTERN ANALYSIS ===\")\n            x_grad_corr, y_grad_corr = analyze_gradient_patterns(\n                input_files[0], output_files[0], sample_idx=sample_idx, channel_idx=channel_idx)\n            \n            print(\"\\n=== INITIAL POOLING EFFECT ANALYSIS ===\")\n            orig_sample, pooled_sample = analyze_initial_pool_effect(\n                input_files[0], sample_idx=sample_idx, channel_idx=channel_idx)\n            \n            print(\"\\n=== FREQUENCY COMPONENT ANALYSIS ===\")\n            freq_data = analyze_frequency_components(\n                input_files[0], output_files[0], sample_idx=sample_idx, channel_idx=channel_idx)\n    \n    # Return key statistics for further use\n    return {\n        'input_stats': {\n            'mean': input_stats_df['mean'].mean(),\n            'std': input_stats_df['std'].mean(),\n            'min': input_stats_df['min'].min(),\n            'max': input_stats_df['max'].max(),\n        },\n        'output_stats': {\n            'mean': output_stats_df['mean'].mean(),\n            'std': output_stats_df['std'].mean(),\n            'min': output_stats_df['min'].min(),\n            'max': output_stats_df['max'].max(),\n        },\n        'input_shape': input_shape,\n        'output_shape': output_shape\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:21:05.726817Z","iopub.execute_input":"2025-05-12T16:21:05.727076Z","iopub.status.idle":"2025-05-12T16:21:05.739608Z","shell.execute_reply.started":"2025-05-12T16:21:05.727053Z","shell.execute_reply":"2025-05-12T16:21:05.739058Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# if __name__ == \"__main__\":\n    # Run data analysis on the provided dataset\n#    data_path = \"/kaggle/input/waveform-inversion\"  # Update with your data path\n#    stats = run_data_analysis(data_path, n_samples=5)\n    \n#    print(\"\\n=== ANALYSIS COMPLETE ===\")\n#    print(\"\\nSummary of findings:\")\n#    print(f\"Input data shape: {stats['input_shape']}\")\n#    print(f\"Output data shape: {stats['output_shape']}\")\n#    print(f\"Input data - Min: {stats['input_stats']['min']:.4f}, Max: {stats['input_stats']['max']:.4f}\")\n#    print(f\"Input data - Mean: {stats['input_stats']['mean']:.4f}, Std: {stats['input_stats']['std']:.4f}\")\n#    print(f\"Output data - Min: {stats['output_stats']['min']:.4f}, Max: {stats['output_stats']['max']:.4f}\")\n#    print(f\"Output data - Mean: {stats['output_stats']['mean']:.4f}, Std: {stats['output_stats']['std']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:21:05.740292Z","iopub.execute_input":"2025-05-12T16:21:05.740484Z","iopub.status.idle":"2025-05-12T16:21:05.750815Z","shell.execute_reply.started":"2025-05-12T16:21:05.740470Z","shell.execute_reply":"2025-05-12T16:21:05.750260Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"# Modeling Functions","metadata":{}},{"cell_type":"code","source":"# Improved dataset classes with normalization\nclass SeismicDataset(Dataset):\n    def __init__(self, inputs_files, output_files, n_examples_per_file=500, normalize=True, transform=None):\n        assert len(inputs_files) == len(output_files)\n        self.inputs_files = inputs_files\n        self.output_files = output_files\n        self.n_examples_per_file = n_examples_per_file\n        self.normalize = normalize\n        self.transform = transform\n        \n        # Calculate normalization statistics if needed\n        if normalize:\n            self.input_stats, self.output_stats = self._calculate_stats()\n\n    def _calculate_stats(self):\n        \"\"\"Calculate mean and std for normalization\"\"\"\n        # Sample a subset of files to calculate statistics\n        input_means, input_stds = [], []\n        output_means, output_stds = [], []\n        \n        sample_size = min(10, len(self.inputs_files))\n        \n        print(f\"Calculating statistics from {sample_size} files...\")\n        for i in range(sample_size):\n            X = np.load(self.inputs_files[i])\n            y = np.load(self.output_files[i])\n            \n            # Calc stats for input\n            input_means.append(np.mean(X))\n            input_stds.append(np.std(X))\n            \n            # Calc stats for output\n            output_means.append(np.mean(y))\n            output_stds.append(np.std(y))\n        \n        input_mean = np.mean(input_means)\n        input_std = np.std(input_stds) if np.std(input_stds) > 0 else 1.0\n        output_mean = np.mean(output_means)\n        output_std = np.std(output_stds) if np.std(output_stds) > 0 else 1.0\n        \n        print(f\"Input stats - Mean: {input_mean:.4f}, Std: {input_std:.4f}\")\n        print(f\"Output stats - Mean: {output_mean:.4f}, Std: {output_std:.4f}\")\n        \n        return (input_mean, input_std), (output_mean, output_std)\n\n    def __len__(self):\n        return len(self.inputs_files) * self.n_examples_per_file\n\n    def __getitem__(self, idx):\n        # Calculate file offset and sample offset within file\n        file_idx = idx // self.n_examples_per_file\n        sample_idx = idx % self.n_examples_per_file\n    \n        X = np.load(self.inputs_files[file_idx], mmap_mode='r')\n        y = np.load(self.output_files[file_idx], mmap_mode='r')\n    \n        try:\n            X_sample, y_sample = X[sample_idx].copy(), y[sample_idx].copy()\n            \n            # Apply normalization if enabled\n            if self.normalize:\n                X_sample = (X_sample - self.input_stats[0]) / (self.input_stats[1] + 1e-8)\n                y_sample = (y_sample - self.output_stats[0]) / (self.output_stats[1] + 1e-8)\n            \n            # Apply any additional transforms\n            if self.transform:\n                X_sample, y_sample = self.transform(X_sample, y_sample)\n                \n            # Ensure contiguous memory layout to avoid negative stride issues\n            X_sample = np.ascontiguousarray(X_sample)\n            y_sample = np.ascontiguousarray(y_sample)\n                \n            # Convert to torch tensors\n            X_sample = torch.from_numpy(X_sample).float()\n            y_sample = torch.from_numpy(y_sample).float()\n                \n            return X_sample, y_sample\n        finally:\n            del X, y\n\nclass TestDataset(Dataset):\n    def __init__(self, test_files, normalize=True, input_stats=None):\n        self.test_files = test_files\n        self.normalize = normalize\n        self.input_stats = input_stats\n\n    def __len__(self):\n        return len(self.test_files)\n\n    def __getitem__(self, i):\n        test_file = self.test_files[i]\n        X = np.load(test_file)\n        \n        if self.normalize and self.input_stats:\n            X = (X - self.input_stats[0]) / (self.input_stats[1] + 1e-8)\n        \n        X = torch.from_numpy(X).float()\n        return X, test_file.stem","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:21:05.751672Z","iopub.execute_input":"2025-05-12T16:21:05.752418Z","iopub.status.idle":"2025-05-12T16:21:05.764016Z","shell.execute_reply.started":"2025-05-12T16:21:05.752400Z","shell.execute_reply":"2025-05-12T16:21:05.763372Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Data augmentation class with fixed padding operation\nclass SeismicAugmentation:\n    def __init__(self, \n                 flip_prob=0.5, \n                 noise_prob=0.3, \n                 noise_level=0.05,\n                 shift_prob=0.3,\n                 max_shift=5):\n        self.flip_prob = flip_prob\n        self.noise_prob = noise_prob\n        self.noise_level = noise_level\n        self.shift_prob = shift_prob\n        self.max_shift = max_shift\n\n    def __call__(self, X, y):\n        # Make a copy to avoid modifying the original data\n        X = X.copy()\n        y = y.copy()\n        \n        # Horizontal flip\n        if np.random.random() < self.flip_prob:\n            X = np.flip(X, axis=1).copy()  # Use np.flip and make a copy to avoid negative strides\n            y = np.flip(y, axis=1).copy()  # Use np.flip and make a copy to avoid negative strides\n        \n        # Add random noise to input\n        if np.random.random() < self.noise_prob:\n            noise = np.random.normal(0, self.noise_level, X.shape)\n            X = X + noise\n        \n        # Random time shift (along time axis, usually dim 0)\n        if np.random.random() < self.shift_prob:\n            shift = np.random.randint(-self.max_shift, self.max_shift + 1)\n            \n            # Only apply shift if it's non-zero\n            if shift != 0:\n                # Create padding dimensions based on the array's actual shape\n                pad_width = [(0, 0)] * X.ndim  # Initialize with no padding for all dimensions\n                \n                if shift > 0:\n                    # Pad at the beginning of the second dimension (columns/width)\n                    pad_width[1] = (shift, 0)\n                    X = np.pad(X, pad_width, mode='constant')[:, :-shift]\n                else:  # shift < 0\n                    # Pad at the end of the second dimension\n                    pad_width[1] = (0, -shift)\n                    X = np.pad(X, pad_width, mode='constant')[:, -shift:]\n        \n        return X, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:21:05.765626Z","iopub.execute_input":"2025-05-12T16:21:05.766023Z","iopub.status.idle":"2025-05-12T16:21:05.777844Z","shell.execute_reply.started":"2025-05-12T16:21:05.766004Z","shell.execute_reply":"2025-05-12T16:21:05.777084Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Attention Gate for U-Net\nclass AttentionGate(nn.Module):\n    \"\"\"Attention Gate for U-Net architecture\"\"\"\n    def __init__(self, F_g, F_l, F_int):\n        super(AttentionGate, self).__init__()\n        \n        # Print dimensions for debugging\n        print(f\"Creating AttentionGate with F_g={F_g}, F_l={F_l}, F_int={F_int}\")\n        \n        self.W_g = nn.Sequential(\n            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n        self.psi = nn.Sequential(\n            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, g, x):\n        # Print shapes for debugging\n        # print(f\"g shape: {g.shape}, x shape: {x.shape}\")\n        \n        g1 = self.W_g(g)\n        x1 = self.W_x(x)\n        \n        # Ensure shapes match for addition\n        if g1.shape[2:] != x1.shape[2:]:\n            # Resize g1 to match x1's spatial dimensions\n            g1 = F.interpolate(g1, size=x1.shape[2:], mode='bilinear', align_corners=False)\n        \n        psi = self.relu(g1 + x1)\n        psi = self.psi(psi)\n        \n        # Ensure psi has the same spatial dimensions as x\n        if psi.shape[2:] != x.shape[2:]:\n            psi = F.interpolate(psi, size=x.shape[2:], mode='bilinear', align_corners=False)\n            \n        return x * psi\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:21:05.778663Z","iopub.execute_input":"2025-05-12T16:21:05.778906Z","iopub.status.idle":"2025-05-12T16:21:05.791218Z","shell.execute_reply.started":"2025-05-12T16:21:05.778886Z","shell.execute_reply":"2025-05-12T16:21:05.790453Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Squeeze-and-Excitation block\nclass SEBlock(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SEBlock, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y.expand_as(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:21:05.791958Z","iopub.execute_input":"2025-05-12T16:21:05.792209Z","iopub.status.idle":"2025-05-12T16:21:05.802320Z","shell.execute_reply.started":"2025-05-12T16:21:05.792189Z","shell.execute_reply":"2025-05-12T16:21:05.801639Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Improved Residual Double Conv Block with SE\nclass ResidualDoubleConv(nn.Module):\n    \"\"\"(Convolution => [BN] => ReLU) * 2 + Residual Connection + SE Block\"\"\"\n\n    def __init__(self, in_channels, out_channels, mid_channels=None, use_se=True):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n\n        # First convolution layer\n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_channels)\n        self.relu = nn.LeakyReLU(negative_slope=0.01, inplace=True)  # Using LeakyReLU\n\n        # Second convolution layer\n        self.conv2 = nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        # SE block\n        self.use_se = use_se\n        if use_se:\n            self.se = SEBlock(out_channels, reduction=16)\n\n        # Shortcut connection to handle potential channel mismatch\n        if in_channels == out_channels:\n            self.shortcut = nn.Identity()\n        else:\n            # Projection shortcut: 1x1 conv + BN to match output channels\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        identity = x  # Store the input for the residual connection\n\n        # First conv block\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        # Second conv block (without final ReLU yet)\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        # Apply SE block if enabled\n        if self.use_se:\n            out = self.se(out)\n\n        # Apply shortcut to the identity path\n        identity_mapped = self.shortcut(identity)\n\n        # Add the residual connection\n        out += identity_mapped\n\n        # Apply final ReLU\n        out = self.relu(out)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:21:05.803094Z","iopub.execute_input":"2025-05-12T16:21:05.803302Z","iopub.status.idle":"2025-05-12T16:21:05.814527Z","shell.execute_reply.started":"2025-05-12T16:21:05.803288Z","shell.execute_reply":"2025-05-12T16:21:05.813982Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Improved Up block with attention\nclass Up(nn.Module):\n    \"\"\"Upscaling then ResidualDoubleConv with Attention\"\"\"\n\n    def __init__(self, in_channels, out_channels, bilinear=True, use_attention=True):\n        super().__init__()\n        self.bilinear = bilinear\n        self.use_attention = use_attention\n\n        # Print dimensions for debugging\n        # print(f\"Creating Up block with in_channels={in_channels}, out_channels={out_channels}, bilinear={bilinear}\")\n\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)\n            # With bilinear upsampling, the number of channels doesn't change\n            conv_in_channels = in_channels + out_channels  # Skip connection + upsampled features\n            self.conv = ResidualDoubleConv(conv_in_channels, out_channels)\n            \n            # For attention gate, the g input is the upsampled feature map (in_channels)\n            # and the x input is the skip connection (out_channels)\n            if use_attention:\n                self.attention = AttentionGate(F_g=in_channels, F_l=out_channels, F_int=out_channels // 2)\n\n        else:  # Using ConvTranspose2d\n            # ConvTranspose halves the channels: in_channels -> in_channels // 2\n            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n            # Input channels to ResidualDoubleConv\n            conv_in_channels = in_channels // 2 + out_channels  # After transpose conv + skip connection\n            self.conv = ResidualDoubleConv(conv_in_channels, out_channels)\n            \n            # For attention gate, the g input is the upsampled feature map (in_channels/2)\n            # and the x input is the skip connection (out_channels)\n            if use_attention:\n                self.attention = AttentionGate(F_g=in_channels // 2, F_l=out_channels, F_int=out_channels // 2)\n\n    def forward(self, x1, x2):\n        # x1 is the feature map from the layer below (needs upsampling)\n        # x2 is the skip connection from the corresponding encoder layer\n        \n        # Print shapes for debugging\n        # print(f\"Before up: x1 shape: {x1.shape}, x2 shape: {x2.shape}\")\n        \n        x1 = self.up(x1)\n        \n        # Print shapes after upsampling for debugging\n        # print(f\"After up: x1 shape: {x1.shape}\")\n        \n        # Input is CHW\n        diffY = x2.size(2) - x1.size(2)\n        diffX = x2.size(3) - x1.size(3)\n    \n        # Handle case where x1 needs padding (diffY/diffX > 0)\n        if diffY > 0 or diffX > 0:\n            # Pad format: (padding_left, padding_right, padding_top, padding_bottom)\n            x1 = F.pad(\n                x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2]\n            )\n        # Handle case where x1 is larger than x2 (diffY/diffX < 0)\n        elif diffY < 0 or diffX < 0:\n            # Crop x1 to match x2's spatial dimensions\n            # Calculate cropping amounts\n            crop_y = -diffY\n            crop_x = -diffX\n            \n            # Crop x1 to match x2\n            x1 = x1[:, :, \n                    crop_y//2:crop_y//2 + x2.size(2),\n                    crop_x//2:crop_x//2 + x2.size(3)]\n        \n        # Print shapes after padding/cropping for debugging\n        # print(f\"After adjustment: x1 shape: {x1.shape}, x2 shape: {x2.shape}\")\n        \n        # Apply attention if enabled\n        if self.use_attention:\n            x2_att = self.attention(x1, x2)\n        else:\n            x2_att = x2\n    \n        # Concatenate along the channel dimension\n        x = torch.cat([x2_att, x1], dim=1)\n        \n        # Print shape before conv for debugging\n        # print(f\"Before conv: concatenated shape: {x.shape}\")\n        \n        return self.conv(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:21:05.815317Z","iopub.execute_input":"2025-05-12T16:21:05.815542Z","iopub.status.idle":"2025-05-12T16:21:05.828410Z","shell.execute_reply.started":"2025-05-12T16:21:05.815525Z","shell.execute_reply":"2025-05-12T16:21:05.827708Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# Improved UNet architecture\nclass UNet(nn.Module):\n    \"\"\"U-Net architecture implementation with Residual Blocks, SE, and Attention\"\"\"\n\n    def __init__(\n        self,\n        n_channels=5,\n        n_classes=1,\n        init_features=64,  # Increased from 32 to 64\n        depth=5,\n        bilinear=True,\n        use_attention=True,\n        use_se=True,\n    ):\n        super().__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n        self.depth = depth\n        self.use_attention = use_attention\n        self.use_se = use_se\n\n        # Improved approach for multi-channel data\n        self.initial_pool = nn.Sequential(\n            # Use depthwise separable convolution to process each channel independently first\n            nn.Conv2d(5, 5, kernel_size=(14, 1), stride=(14, 1), groups=5, bias=False),\n            nn.BatchNorm2d(5),\n            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n            # Then combine channel information with pointwise convolution\n            nn.Conv2d(5, init_features, kernel_size=1, bias=False),\n            nn.BatchNorm2d(init_features),\n            nn.LeakyReLU(negative_slope=0.1, inplace=True)\n        )\n\n        # --- Encoder ---\n        self.encoder_convs = nn.ModuleList()  # Store conv blocks\n        self.encoder_pools = nn.ModuleList()  # Store pool layers\n\n        # Initial conv block (no pooling before it)\n        # Use ResidualDoubleConv for the initial convolution block\n        self.inc = ResidualDoubleConv(n_channels, init_features, use_se=use_se)\n        self.encoder_convs.append(self.inc)\n\n        current_features = init_features\n        for _ in range(depth):\n            # Define convolution block for this stage\n            conv = ResidualDoubleConv(current_features, current_features * 2, use_se=use_se)\n            # Define pooling layer for this stage\n            pool = nn.MaxPool2d(2)\n            self.encoder_convs.append(conv)\n            self.encoder_pools.append(pool)\n            current_features *= 2\n\n        # --- Bottleneck ---\n        # Use ResidualDoubleConv for the bottleneck\n        self.bottleneck = ResidualDoubleConv(current_features, current_features, use_se=use_se)\n\n        # --- Decoder ---\n        self.decoder_blocks = nn.ModuleList()\n        # Input features start from bottleneck output features\n        # Output features at each stage are halved\n        for _ in range(depth):\n            # Up block uses ResidualDoubleConv internally and handles channels\n            up_block = Up(current_features, current_features // 2, \n                          bilinear=bilinear, use_attention=use_attention)\n            self.decoder_blocks.append(up_block)\n            current_features //= 2  # Halve features for next Up block input\n\n        # --- Output Layer ---\n        # Input features are the output features of the last Up block\n        self.outc = OutConv(current_features, n_classes)\n\n    def _pad_or_crop(self, x, target_h=70, target_w=70):\n        \"\"\"Pads or crops input tensor x to target height and width.\"\"\"\n        _, _, h, w = x.shape\n        # Pad Height if needed\n        if h < target_h:\n            pad_top = (target_h - h) // 2\n            pad_bottom = target_h - h - pad_top\n            x = F.pad(x, (0, 0, pad_top, pad_bottom))  # Pad height only\n            h = target_h\n        # Pad Width if needed\n        if w < target_w:\n            pad_left = (target_w - w) // 2\n            pad_right = target_w - w - pad_left\n            x = F.pad(x, (pad_left, pad_right, 0, 0))  # Pad width only\n            w = target_w\n        # Crop Height if needed\n        if h > target_h:\n            crop_top = (h - target_h) // 2\n            # Use slicing to crop\n            x = x[:, :, crop_top : crop_top + target_h, :]\n            h = target_h\n        # Crop Width if needed\n        if w > target_w:\n            crop_left = (w - target_w) // 2\n            x = x[:, :, :, crop_left : crop_left + target_w]\n            w = target_w\n        return x\n\n    def forward(self, x):\n        # Initial pooling and resizing\n        # print(f\"Input shape: {x.shape}\")\n        x_pooled = self.initial_pool(x)\n        # print(f\"After initial_pool: {x_pooled.shape}\")\n        x_resized = self._pad_or_crop(x_pooled, target_h=70, target_w=70)\n        # print(f\"After pad_or_crop: {x_resized.shape}\")\n    \n        # --- Encoder Path ---\n        skip_connections = []\n        xi = x_resized\n    \n        # Apply initial conv (inc)\n        xi = self.encoder_convs[0](xi)\n        # print(f\"After initial conv: {xi.shape}\")\n        skip_connections.append(xi)  # Store output of inc\n    \n        # Apply subsequent encoder convs and pools\n        # self.depth is the number of pooling layers\n        for i in range(self.depth):\n            # Apply conv block for this stage\n            xi = self.encoder_convs[i+1](xi)\n            # print(f\"After encoder conv {i+1}: {xi.shape}\")\n            # Store skip connection *before* pooling\n            skip_connections.append(xi)\n            # Apply pooling layer for this stage\n            xi = self.encoder_pools[i](xi)\n            # print(f\"After pool {i+1}: {xi.shape}\")\n    \n        # Apply bottleneck conv\n        xi = self.bottleneck(xi)\n        # print(f\"After bottleneck: {xi.shape}\")\n    \n        # --- Decoder Path ---\n        xu = xi  # Start with bottleneck output\n        # Iterate through decoder blocks and corresponding skip connections in reverse\n        for i, block in enumerate(self.decoder_blocks):\n            # Determine the correct skip connection index from the end\n            skip_index = self.depth - 1 - i\n            skip = skip_connections[skip_index]\n            # print(f\"Decoder {i}, xu shape: {xu.shape}, skip shape: {skip.shape}\")\n            xu = block(xu, skip)  # Up block combines xu (from below) and skip\n            # print(f\"After decoder block {i}: {xu.shape}\")\n    \n        # --- Final Output ---\n        logits = self.outc(xu)\n        # print(f\"Logits shape: {logits.shape}\")\n        \n        # Apply scaling and offset specific to the problem's target range\n        output = logits * 1000.0 + 1500.0\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:21:05.829197Z","iopub.execute_input":"2025-05-12T16:21:05.829439Z","iopub.status.idle":"2025-05-12T16:21:05.843123Z","shell.execute_reply.started":"2025-05-12T16:21:05.829417Z","shell.execute_reply":"2025-05-12T16:21:05.842510Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Output Convolution\nclass OutConv(nn.Module):\n    \"\"\"1x1 Convolution for the output layer\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:21:05.844459Z","iopub.execute_input":"2025-05-12T16:21:05.844726Z","iopub.status.idle":"2025-05-12T16:21:05.854241Z","shell.execute_reply.started":"2025-05-12T16:21:05.844709Z","shell.execute_reply":"2025-05-12T16:21:05.853593Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# Custom Loss Functions\nclass FocalLoss(nn.Module):\n    \"\"\"\n    Focal Loss for handling class imbalance\n    \"\"\"\n    def __init__(self, gamma=2.0, alpha=None, reduction='mean', eps=1e-6):  # Added eps\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n        self.reduction = reduction\n        self.eps = eps  # Small epsilon to prevent numerical instability\n\n    def forward(self, input, target):\n        # For regression tasks, we can adapt focal loss\n        loss = F.mse_loss(input, target, reduction='none')\n        \n        # Apply focal weighting\n        focal_weight = torch.exp(-loss)\n        focal_weight = torch.clamp(focal_weight, min=self.eps, max=1-self.eps) # Clamp to avoid log(0)\n        focal_loss = torch.pow(1 - focal_weight, self.gamma) * loss\n        \n        if self.alpha is not None:\n            # Apply alpha weighting if provided\n            focal_loss = self.alpha * focal_loss\n        \n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        elif self.reduction == 'sum':\n            return focal_loss.sum()\n        else:\n            return focal_loss\n\nclass GradientLoss(nn.Module):\n    \"\"\"\n    Loss that incorporates gradients of the prediction and target\n    to preserve structural information in seismic imaging\n    \"\"\"\n    def __init__(self, lambda_grad=0.5, reduction='mean'):\n        super(GradientLoss, self).__init__()\n        self.lambda_grad = lambda_grad\n        self.reduction = reduction\n    \n    def forward(self, input, target):\n        # MSE loss\n        mse_loss = F.mse_loss(input, target, reduction='none')\n        \n        # Calculate gradients using Sobel filters\n        # Horizontal gradient\n        h_kernel = torch.tensor([[[[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]]], \n                                 dtype=input.dtype, device=input.device)\n        h_kernel = h_kernel.repeat(input.size(1), 1, 1, 1)\n        \n        # Vertical gradient\n        v_kernel = torch.tensor([[[[-1, -2, -1], [0, 0, 0], [1, 2, 1]]]], \n                                 dtype=input.dtype, device=input.device)\n        v_kernel = v_kernel.repeat(input.size(1), 1, 1, 1)\n        \n        # Calculate gradients\n        input_h_grad = F.conv2d(input, h_kernel, padding=1, groups=input.size(1))\n        input_v_grad = F.conv2d(input, v_kernel, padding=1, groups=input.size(1))\n        target_h_grad = F.conv2d(target, h_kernel, padding=1, groups=target.size(1))\n        target_v_grad = F.conv2d(target, v_kernel, padding=1, groups=target.size(1))\n        \n        # Calculate gradient loss\n        h_grad_loss = F.mse_loss(input_h_grad, target_h_grad, reduction='none')\n        v_grad_loss = F.mse_loss(input_v_grad, target_v_grad, reduction='none')\n        grad_loss = h_grad_loss + v_grad_loss\n        \n        # Combine MSE and gradient loss\n        combined_loss = mse_loss + self.lambda_grad * grad_loss\n        \n        if self.reduction == 'mean':\n            return combined_loss.mean()\n        elif self.reduction == 'sum':\n            return combined_loss.sum()\n        else:\n            return combined_loss\n\nclass CombinedLoss(nn.Module):\n    \"\"\"\n    Combine multiple loss functions\n    \"\"\"\n    def __init__(self, losses, weights=None):\n        super(CombinedLoss, self).__init__()\n        self.losses = nn.ModuleList(losses)\n        self.weights = weights if weights is not None else [1.0] * len(losses)\n        assert len(self.losses) == len(self.weights), \"Number of losses and weights must match\"\n\n    def forward(self, input, target):\n        total_loss = 0\n        for i, loss in enumerate(self.losses):\n            total_loss += self.weights[i] * loss(input, target)\n        return total_loss\n\nclass FrequencyDomainLoss(nn.Module):\n    \"\"\"\n    Loss calculated in the frequency domain to stabilize training.\n    \"\"\"\n    def __init__(self, reduction='mean', loss_type='mse', eps=1e-6, lambda_freq=0.3):\n        super(FrequencyDomainLoss, self).__init__()\n        self.reduction = reduction\n        self.loss_type = loss_type\n        self.eps = eps  # Small epsilon for numerical stability\n        self.lambda_freq = lambda_freq\n\n    def forward(self, input, target):\n        # 1. Transform to frequency domain (using rfft2 for real-valued input)\n        input_fft = torch.fft.rfft2(input.float())  # Ensure float type\n        target_fft = torch.fft.rfft2(target.float())\n\n        # 2. Calculate the loss, handling complex numbers correctly\n        if self.loss_type == 'mse':\n            # Calculate MSE on magnitude, real, and imaginary parts, then combine.\n            mag_loss = F.mse_loss(torch.abs(input_fft), torch.abs(target_fft), reduction='none')\n            real_loss = F.mse_loss(input_fft.real, target_fft.real, reduction='none')\n            imag_loss = F.mse_loss(input_fft.imag, target_fft.imag, reduction='none')\n            loss = mag_loss + real_loss + imag_loss # Combine losses.  You can weight them differently if needed.\n\n        elif self.loss_type == 'mae':\n            mag_loss = F.l1_loss(torch.abs(input_fft), torch.abs(target_fft), reduction='none')\n            real_loss = F.l1_loss(input_fft.real, target_fft.real, reduction='none')\n            imag_loss = F.l1_loss(input_fft.imag, target_fft.imag, reduction='none')\n            loss = mag_loss + real_loss + imag_loss\n\n        elif self.loss_type == 'magnitude':\n            input_mag = torch.abs(input_fft)\n            target_mag = torch.abs(target_fft)\n            loss = F.mse_loss(input_mag, target_mag, reduction='none')\n        elif self.loss_type == 'phase':\n            input_phase = torch.angle(input_fft)\n            target_phase = torch.angle(target_fft)\n            loss = F.mse_loss(input_phase, target_phase, reduction='none')\n        elif self.loss_type == 'complex_mse':\n            # Complex MSE: Sum of squares of real and imaginary differences\n            real_loss = F.mse_loss(input_fft.real, target_fft.real, reduction='none')\n            imag_loss = F.mse_loss(input_fft.imag, target_fft.imag, reduction='none')\n            loss = real_loss + imag_loss\n        else:\n            raise ValueError(f\"Unsupported loss type: {self.loss_type}\")\n\n        # 3. Apply reduction\n        if self.reduction == 'mean':\n            return loss.mean()\n        elif self.reduction == 'sum':\n            return loss.sum()\n        else:\n            return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:21:05.854946Z","iopub.execute_input":"2025-05-12T16:21:05.855263Z","iopub.status.idle":"2025-05-12T16:21:05.872724Z","shell.execute_reply.started":"2025-05-12T16:21:05.855239Z","shell.execute_reply":"2025-05-12T16:21:05.872197Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# Training function with mixed precision\ndef train_with_mixed_precision(model, train_loader, valid_loader, criterion, optimizer, scheduler, config, device):\n    \"\"\"\n    Training function with mixed precision and validation\n    \"\"\"\n    # Initialize gradient scaler for mixed precision\n    try:\n        # New way (PyTorch 2.0+)\n        from torch.amp import GradScaler\n        scaler = GradScaler(device_type='cuda')\n    except TypeError:\n        # Old way (PyTorch < 2.0)\n        from torch.cuda.amp import GradScaler\n        scaler = GradScaler()\n    \n    best_valid_loss = float('inf')\n    early_stop_counter = 0\n    history = {'train_loss': [], 'valid_loss': []}\n    \n    for epoch in range(config['max_epochs']):\n        start_time = time.time()\n        \n        # Training phase\n        model.train()\n        train_losses = []\n        \n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = data.to(device), target.to(device)\n            \n            optimizer.zero_grad()\n            \n            # Use autocast for mixed precision\n            with torch.autocast(device_type='cuda', dtype=torch.float16):\n                output = model(data)\n                loss = criterion(output, target)\n            \n            # Scale the loss and call backward\n            scaler.scale(loss).backward()\n            \n            # Gradient clipping to prevent exploding gradients\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            # Perform optimization step with scaler\n            scaler.step(optimizer)\n            scaler.update()\n            \n            train_losses.append(loss.item())\n            \n            if batch_idx % config['print_freq'] == 0:\n                print(f'Epoch: {epoch+1}/{config[\"max_epochs\"]} '\n                      f'[{batch_idx*len(data)}/{len(train_loader.dataset)} '\n                      f'({100. * batch_idx / len(train_loader):.0f}%)]\\t'\n                      f'Loss: {loss.item():.6f}')\n        \n        avg_train_loss = sum(train_losses) / len(train_losses)\n        history['train_loss'].append(avg_train_loss)\n        \n        # Validation phase\n        model.eval()\n        valid_losses = []\n        \n        with torch.no_grad():\n            for data, target in valid_loader:\n                data, target = data.to(device), target.to(device)\n                # No need for autocast in validation as we're not training\n                output = model(data)\n                loss = criterion(output, target)\n                valid_losses.append(loss.item())\n        \n        avg_valid_loss = sum(valid_losses) / len(valid_losses)\n        history['valid_loss'].append(avg_valid_loss)\n        \n        epoch_time = time.time() - start_time\n        \n        print(f'Epoch: {epoch+1} Train Loss: {avg_train_loss:.6f} Valid Loss: {avg_valid_loss:.6f} '\n              f'Time: {epoch_time:.1f}s LR: {optimizer.param_groups[0][\"lr\"]:.8f}')\n        \n        # Learning rate scheduler step based on validation loss\n        scheduler.step(avg_valid_loss)\n        \n        # Save the best model\n        if avg_valid_loss < best_valid_loss:\n            best_valid_loss = avg_valid_loss\n            early_stop_counter = 0\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n                'loss': best_valid_loss,\n            }, 'best_model.pth')\n            print(f\"Model saved with validation loss: {best_valid_loss:.6f}\")\n        else:\n            early_stop_counter += 1\n            \n        # Early stopping check\n        if early_stop_counter >= config['es_epochs']:\n            print(f\"Early stopping triggered after {epoch+1} epochs\")\n            break\n    \n    return model, history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:21:05.873368Z","iopub.execute_input":"2025-05-12T16:21:05.873540Z","iopub.status.idle":"2025-05-12T16:21:05.886673Z","shell.execute_reply.started":"2025-05-12T16:21:05.873527Z","shell.execute_reply":"2025-05-12T16:21:05.886082Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# K-fold cross-validation function\ndef k_fold_cross_validation(model_class, inputs_files, output_files, criterion, config, n_folds=5):\n    \"\"\"\n    Perform k-fold cross-validation\n    \"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    kf = KFold(n_splits=n_folds, shuffle=True, random_state=config['seed'])\n    \n    # Convert to numpy arrays for easier indexing\n    inputs_files = np.array(inputs_files)\n    output_files = np.array(output_files)\n    \n    fold_results = []\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(inputs_files)):\n        print(f\"Training fold {fold+1}/{n_folds}\")\n        \n        # Get files for this fold\n        train_input_files = inputs_files[train_idx]\n        train_output_files = output_files[train_idx]\n        val_input_files = inputs_files[val_idx]\n        val_output_files = output_files[val_idx]\n        \n        # Create datasets and dataloaders\n        # Add data augmentation to training\n        transform = SeismicAugmentation(flip_prob=0.5, noise_prob=0.3, noise_level=0.05)\n        \n        train_dataset = SeismicDataset(train_input_files, train_output_files, \n                                      normalize=True, transform=transform)\n        val_dataset = SeismicDataset(val_input_files, val_output_files, \n                                    normalize=True, transform=None)\n        \n        train_loader = DataLoader(\n            train_dataset, \n            batch_size=config['batch_size'],\n            shuffle=True,\n            num_workers=4,\n            pin_memory=True\n        )\n        \n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=config['batch_size'],\n            shuffle=False,\n            num_workers=4,\n            pin_memory=True\n        )\n        \n        # Initialize model, optimizer, and scheduler for this fold\n        model = model_class(**config['model']['unet_params'])\n        model.to(device)\n        \n        optimizer = torch.optim.AdamW(\n            model.parameters(),\n            lr=config['optimizer']['lr'],\n            weight_decay=config['optimizer']['weight_decay']\n        )\n        \n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer,\n            **config['scheduler']['params']\n        )\n        \n        # Train model\n        model, history = train_with_mixed_precision(model, train_loader, val_loader, \n                                                  criterion, optimizer, scheduler, \n                                                  config, device)\n        \n        # Save fold results\n        fold_results.append({\n            'fold': fold + 1,\n            'best_val_loss': min(history['valid_loss']),\n            'history': history\n        })\n        \n        # Save fold model\n        torch.save({\n            'fold': fold + 1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'history': history,\n        }, f'model_fold_{fold+1}.pth')\n    \n    # Calculate average performance across folds\n    avg_best_val_loss = sum(result['best_val_loss'] for result in fold_results) / n_folds\n    print(f\"Average best validation loss across {n_folds} folds: {avg_best_val_loss:.6f}\")\n    \n    return fold_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:21:05.889097Z","iopub.execute_input":"2025-05-12T16:21:05.889319Z","iopub.status.idle":"2025-05-12T16:21:05.899767Z","shell.execute_reply.started":"2025-05-12T16:21:05.889305Z","shell.execute_reply":"2025-05-12T16:21:05.899281Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# Function to create test predictions\ndef predict(model, test_loader, device):\n    \"\"\"Generate predictions for test data\"\"\"\n    model.eval()\n    predictions = {}\n    \n    with torch.no_grad():\n        for data, file_names in test_loader:\n            data = data.to(device)\n            outputs = model(data)\n            \n            # Denormalize outputs if needed (assuming model outputs already in correct range)\n            # Convert tensors to numpy arrays\n            outputs_cpu = outputs.cpu().numpy()\n            \n            # Store predictions by file name\n            for i, file_name in enumerate(file_names):\n                predictions[file_name] = outputs_cpu[i]\n    \n    return predictions\n\n# Function to create model ensemble\ndef ensemble_predict(models, test_loader, device):\n    \"\"\"Generate predictions using an ensemble of models\"\"\"\n    for model in models:\n        model.eval()\n    \n    predictions = {}\n    \n    with torch.no_grad():\n        for data, file_names in test_loader:\n            data = data.to(device)\n            \n            # Initialize outputs tensor\n            ensemble_outputs = None\n            \n            # Accumulate predictions from each model\n            for model in models:\n                outputs = model(data)\n                \n                if ensemble_outputs is None:\n                    ensemble_outputs = outputs\n                else:\n                    ensemble_outputs += outputs\n            \n            # Average predictions\n            ensemble_outputs /= len(models)\n            \n            # Convert tensors to numpy arrays\n            outputs_cpu = ensemble_outputs.cpu().numpy()\n            \n            # Store predictions by file name\n            for i, file_name in enumerate(file_names):\n                predictions[file_name] = outputs_cpu[i]\n    \n    return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:21:05.900464Z","iopub.execute_input":"2025-05-12T16:21:05.900908Z","iopub.status.idle":"2025-05-12T16:21:05.911755Z","shell.execute_reply.started":"2025-05-12T16:21:05.900887Z","shell.execute_reply":"2025-05-12T16:21:05.910999Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# Function to create TTA (Test Time Augmentation) predictions\ndef tta_predict(model, test_loader, device, n_augmentations=5):\n    \"\"\"Generate predictions using Test Time Augmentation\"\"\"\n    model.eval()\n    predictions = {}\n    \n    # Create augmentation object\n    augmentation = SeismicAugmentation(flip_prob=1.0, noise_prob=0.0, noise_level=0.0)\n    \n    with torch.no_grad():\n        for data, file_names in test_loader:\n            # Original prediction\n            data_orig = data.to(device)\n            outputs_orig = model(data_orig)\n            \n            # Initialize outputs tensor with original prediction\n            tta_outputs = outputs_orig.clone()\n            \n            # Apply augmentations and accumulate predictions\n            for _ in range(n_augmentations):\n                # Apply horizontal flip augmentation\n                data_np = data.numpy()\n                augmented_data = []\n                \n                for i in range(data_np.shape[0]):\n                    # Flip horizontally\n                    aug_data, _ = augmentation(data_np[i], data_np[i])\n                    augmented_data.append(aug_data)\n                \n                # Convert back to tensor and predict\n                data_aug = torch.tensor(np.array(augmented_data), dtype=torch.float).to(device)\n                outputs_aug = model(data_aug)\n                \n                # Apply inverse augmentation to predictions (flip horizontally)\n                outputs_aug_flipped = torch.flip(outputs_aug, dims=[3])  # flip along width dimension\n                \n                # Accumulate\n                tta_outputs += outputs_aug_flipped\n            \n            # Average predictions\n            tta_outputs /= (n_augmentations + 1)  # +1 for original prediction\n            \n            # Convert tensors to numpy arrays\n            outputs_cpu = tta_outputs.cpu().numpy()\n            \n            # Store predictions by file name\n            for i, file_name in enumerate(file_names):\n                predictions[file_name] = outputs_cpu[i]\n    \n    return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:21:05.912521Z","iopub.execute_input":"2025-05-12T16:21:05.913338Z","iopub.status.idle":"2025-05-12T16:21:05.927638Z","shell.execute_reply.started":"2025-05-12T16:21:05.913316Z","shell.execute_reply":"2025-05-12T16:21:05.926981Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# Main function to run the training and evaluation process\ndef run_training(config):\n    # Set seed for reproducibility\n    set_seed(config['seed'])\n    \n    # Get device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f'Using device: {device}')\n    \n    # Get data files\n    input_files, output_files = get_train_files(config['data_path'])\n    print(f'Found {len(input_files)} input files and {len(output_files)} output files')\n    \n    # Set up loss function\n    # Option 1: Simple MSE Loss\n    # criterion = nn.MSELoss()\n    \n    # Option 2: Combined Loss (MSE + Gradient Loss)\n    mse_loss = nn.MSELoss()\n    gradient_loss = GradientLoss(lambda_grad=0.3)\n    freq_loss = FrequencyDomainLoss(lambda_freq=0.3)\n    \n   # Combine all three losses\n    criterion = CombinedLoss(\n        [mse_loss, gradient_loss, freq_loss], \n        [0.8, 0.2, 0.0]  # Adjust weights according to importance\n    )\n    \n    # Choose whether to use K-fold CV or a simple train/val split\n    if config.get('use_kfold', False):\n        # Use K-fold cross-validation\n        print(f\"Using {config['n_folds']}-fold cross-validation\")\n        model_class = globals()[config['model']['name']]\n        fold_results = k_fold_cross_validation(\n            model_class=model_class,\n            inputs_files=input_files,\n            output_files=output_files,\n            criterion=criterion,\n            config=config,\n            n_folds=config.get('n_folds', 5)\n        )\n        \n        # Load the best model from each fold for ensembling\n        models = []\n        for fold in range(config.get('n_folds', 5)):\n            model = globals()[config['model']['name']](**config['model']['unet_params'])\n            checkpoint = torch.load(f'model_fold_{fold+1}.pth')\n            model.load_state_dict(checkpoint['model_state_dict'])\n            model.to(device)\n            models.append(model)\n        \n        # Get test files\n        test_files = [f for f in Path(config['data_path']).rglob('*.npy')\n                    if 'test' in f.stem]\n        \n        # Create test dataset and loader\n        test_dataset = TestDataset(test_files)\n        test_loader = DataLoader(\n            test_dataset,\n            batch_size=config['batch_size'],\n            shuffle=False,\n            num_workers=4,\n            pin_memory=True\n        )\n        \n        # Generate ensemble predictions\n        print(f\"Generating ensemble predictions from {len(models)} models\")\n        predictions = ensemble_predict(models, test_loader, device)\n        \n        # Save predictions\n        for file_name, prediction in predictions.items():\n            np.save(f'prediction_{file_name}.npy', prediction)\n        \n    else:\n        # Use simple train/val split\n        print(\"Using simple train/val split\")\n        \n        # Shuffle the file lists\n        indices = list(range(len(input_files)))\n        random.shuffle(indices)\n        input_files = [input_files[i] for i in indices]\n        output_files = [output_files[i] for i in indices]\n        \n        # Split into train and validation sets\n        val_size = len(input_files) // config['valid_frac']\n        train_size = len(input_files) // config['train_frac'] if config.get('train_frac', 0) > 0 else None\n        \n        if train_size:\n            val_input_files = input_files[:val_size]\n            val_output_files = output_files[:val_size]\n            train_input_files = input_files[val_size:val_size+train_size]\n            train_output_files = output_files[val_size:val_size+train_size]\n        else:\n            val_input_files = input_files[:val_size]\n            val_output_files = output_files[:val_size]\n            train_input_files = input_files[val_size:]\n            train_output_files = output_files[val_size:]\n        \n        print(f'Training with {len(train_input_files)} files, validating with {len(val_input_files)} files')\n        \n        # Create datasets with data augmentation for training\n        transform = SeismicAugmentation(flip_prob=0.5, noise_prob=0.3, noise_level=0.05)\n        \n        train_dataset = SeismicDataset(train_input_files, train_output_files, \n                                      normalize=True, transform=transform)\n        val_dataset = SeismicDataset(val_input_files, val_output_files, \n                                    normalize=True, transform=None)\n        \n        # Create data loaders\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=config['batch_size'],\n            shuffle=True,\n            num_workers=4,\n            pin_memory=True\n        )\n        \n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=config['batch_size'],\n            shuffle=False,\n            num_workers=4,\n            pin_memory=True\n        )\n        \n        # Create model\n        model_class = globals()[config['model']['name']]\n        model = model_class(**config['model']['unet_params'])\n        model.to(device)\n        \n        # Print model summary\n        print(model)\n        total_params = sum(p.numel() for p in model.parameters())\n        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n        print(f'Total parameters: {total_params:,}')\n        print(f'Trainable parameters: {trainable_params:,}')\n        \n        # Initialize optimizer and scheduler\n        optimizer = torch.optim.AdamW(\n            model.parameters(),\n            lr=config['optimizer']['lr'],\n            weight_decay=config['optimizer']['weight_decay']\n        )\n        \n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer,\n            **config['scheduler']['params']\n        )\n        \n        # Train model with mixed precision\n        model, history = train_with_mixed_precision(\n            model=model,\n            train_loader=train_loader,\n            valid_loader=val_loader,\n            criterion=criterion,\n            optimizer=optimizer,\n            scheduler=scheduler,\n            config=config,\n            device=device\n        )\n        \n        # Plot training history\n        plt.figure(figsize=(10, 5))\n        plt.plot(history['train_loss'], label='Train Loss')\n        plt.plot(history['valid_loss'], label='Validation Loss')\n        plt.title('Training and Validation Loss')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.legend()\n        plt.grid(True)\n        plt.savefig('training_history.png')\n        plt.show()\n        \n        # Get test files\n        test_files = [f for f in Path(config['data_path']).rglob('*.npy')\n                    if 'test' in f.stem]\n        \n        # Create test dataset and loader\n        test_dataset = TestDataset(test_files)\n        test_loader = DataLoader(\n            test_dataset,\n            batch_size=config['batch_size'],\n            shuffle=False,\n            num_workers=4,\n            pin_memory=True\n        )\n        \n        # Load best model for inference\n        best_model = model_class(**config['model']['unet_params'])\n        checkpoint = torch.load('best_model.pth')\n        best_model.load_state_dict(checkpoint['model_state_dict'])\n        best_model.to(device)\n        \n        # Generate predictions with TTA\n        print(\"Generating predictions with Test Time Augmentation\")\n        predictions = tta_predict(best_model, test_loader, device, n_augmentations=5)\n        \n        # Save predictions\n        for file_name, prediction in predictions.items():\n            np.save(f'prediction_{file_name}.npy', prediction)\n    \n    print(\"Training and prediction complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:21:05.928369Z","iopub.execute_input":"2025-05-12T16:21:05.928571Z","iopub.status.idle":"2025-05-12T16:21:05.946628Z","shell.execute_reply.started":"2025-05-12T16:21:05.928547Z","shell.execute_reply":"2025-05-12T16:21:05.945855Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# Save config to YAML file\ndef create_config_file():\n    config = {\n        'data_path': '/kaggle/input/waveform-inversion',\n        'model': {\n            'name': 'UNet',\n            'unet_params': {\n                'n_channels': 5,\n                'n_classes': 1,\n                'init_features': 64,  # Increased from 32\n                'depth': 5,\n                'bilinear': True,\n                'use_attention': True,  # Enable attention\n                'use_se': True,  # Enable SE blocks\n            }\n        },\n        'read_weights': None,\n        'batch_size': 32,  # Reduced from 64 to handle more complex model\n        'print_freq': 100,\n        'max_epochs': 30,  # Increased from 20\n        'es_epochs': 5,  # Early stopping patience\n        'seed': 42,\n        'valid_frac': 5,  # 1/5 of data for validation\n        'train_frac': 0,  # Use all remaining data for training\n        'use_kfold': True,  # Enable K-fold cross-validation\n        'n_folds': 5,  # Number of folds\n        'optimizer': {\n            'lr': 1e-4,\n            'weight_decay': 1e-3,\n        },\n        'scheduler': {\n            'params': {\n                'factor': 0.5,  # More gentle LR reduction\n                'patience': 2,  # Wait longer before reducing LR\n                'min_lr': 1e-6  # Minimum LR\n                #'verbose': False,\n            }\n        }\n    }\n    \n    with open('config.yaml', 'w') as f:\n        yaml.dump(config, f, default_flow_style=False)\n    \n    print(\"Configuration file 'config.yaml' created.\")\n    return config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:21:05.947305Z","iopub.execute_input":"2025-05-12T16:21:05.947539Z","iopub.status.idle":"2025-05-12T16:21:05.957936Z","shell.execute_reply.started":"2025-05-12T16:21:05.947517Z","shell.execute_reply":"2025-05-12T16:21:05.957422Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"# Run Training","metadata":{}},{"cell_type":"code","source":"# usage\nif __name__ == \"__main__\":\n    # Create updated config\n    config = create_config_file()\n    \n    # Save config to YAML file\n    with open('config.yaml', 'w') as f:\n        yaml.dump(config, f, default_flow_style=False)\n    \n    # Run training process\n    run_training(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:21:05.958597Z","iopub.execute_input":"2025-05-12T16:21:05.958830Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Configuration file 'config.yaml' created.\nUsing device: cuda\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Inference and Submission","metadata":{}},{"cell_type":"code","source":"# Inference and submission\ndef create_submission(config, model_path='best_model.pth'):   \n    t0 = time.time()\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f'Using device: {device}')\n    \n    # Get test files\n    test_dir = Path(config.get('test_path', 'input/test'))\n    test_files = list(test_dir.glob(\"*.npy\"))\n    print(f'Found {len(test_files)} test files')\n    \n    # Define column names for submission\n    x_cols = [f\"x_{i}\" for i in range(1, 70, 2)]\n    fieldnames = [\"oid_ypos\"] + x_cols\n    \n    # Load model\n    model_class = globals()[config['model']['name']]\n    model = model_class(**config['model']['unet_params'])\n    \n    # Load from ensemble if using k-fold\n    if config.get('use_kfold', False):\n        print(\"Using ensemble model for predictions\")\n        models = []\n        for fold in range(config.get('n_folds', 5)):\n            fold_model = model_class(**config['model']['unet_params'])\n            checkpoint = torch.load(f'model_fold_{fold+1}.pth', map_location=device)\n            fold_model.load_state_dict(checkpoint['model_state_dict'])\n            fold_model.to(device)\n            fold_model.eval()\n            models.append(fold_model)\n        \n        # Create test dataset\n        test_dataset = TestDataset(test_files, normalize=True)\n        test_loader = DataLoader(\n            test_dataset,\n            batch_size=config['batch_size'] * 2,  # Doubled batch size for inference\n            shuffle=False,\n            num_workers=4,\n            pin_memory=True\n        )\n        \n        # Generate ensemble predictions\n        print(f\"Generating ensemble predictions from {len(models)} models\")\n        \n        # Open submission file\n        with open(\"submission.csv\", \"wt\", newline=\"\") as csvfile:\n            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n            writer.writeheader()\n            \n            # Process each batch\n            for inputs, oids_test in test_loader:\n                inputs = inputs.to(device)\n                \n                # Initialize ensemble outputs\n                ensemble_outputs = None\n                \n                with torch.no_grad():\n                    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n                        # Accumulate predictions from each model\n                        for model in models:\n                            outputs = model(inputs)\n                            \n                            if ensemble_outputs is None:\n                                ensemble_outputs = outputs\n                            else:\n                                ensemble_outputs += outputs\n                        \n                        # Average predictions\n                        ensemble_outputs /= len(models)\n                \n                # Extract predictions\n                y_preds = ensemble_outputs[:, 0].cpu().numpy()\n                \n                # Write to CSV\n                for y_pred, oid_test in zip(y_preds, oids_test):\n                    for y_pos in range(70):\n                        row = dict(zip(x_cols, [y_pred[y_pos, x_pos] for x_pos in range(1, 70, 2)]))\n                        row[\"oid_ypos\"] = f\"{oid_test}_y_{y_pos}\"\n                        writer.writerow(row)\n                \n    else:\n        # Using single model\n        print(\"Using single best model for predictions\")\n        checkpoint = torch.load(model_path, map_location=device)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        model.to(device)\n        model.eval()\n        \n        # Create test dataset\n        test_dataset = TestDataset(test_files, normalize=True)\n        test_loader = DataLoader(\n            test_dataset,\n            batch_size=config['batch_size'] * 2,  # Doubled batch size for inference\n            shuffle=False,\n            num_workers=4,\n            pin_memory=True\n        )\n        \n        # Open submission file\n        with open(\"submission.csv\", \"wt\", newline=\"\") as csvfile:\n            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n            writer.writeheader()\n            \n            # Process each batch\n            for inputs, oids_test in test_loader:\n                inputs = inputs.to(device)\n                \n                with torch.no_grad():\n                    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n                        outputs = model(inputs)\n                \n                # Extract predictions\n                y_preds = outputs[:, 0].cpu().numpy()\n                \n                # Write to CSV\n                for y_pred, oid_test in zip(y_preds, oids_test):\n                    for y_pos in range(70):\n                        row = dict(zip(x_cols, [y_pred[y_pos, x_pos] for x_pos in range(1, 70, 2)]))\n                        row[\"oid_ypos\"] = f\"{oid_test}_y_{y_pos}\"\n                        writer.writerow(row)\n    \n    t1 = time.time() - t0\n    print(f\"Inference Time: {t1:.2f} seconds\")\n    print(f\"Submission file created: submission.csv\")\n\n# Run inference and create submission\nif __name__ == \"__main__\":\n    # Load config\n    with open('config.yaml', 'r') as f:\n        config = yaml.safe_load(f)\n    \n    # Add test path to config if not present\n    if 'test_path' not in config:\n        config['test_path'] = '/kaggle/input/waveform-inversion/test'\n    \n    # Create submission\n    create_submission(config)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}