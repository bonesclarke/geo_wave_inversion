{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T14:53:18.823481Z",
     "iopub.status.busy": "2025-04-25T14:53:18.823205Z",
     "iopub.status.idle": "2025-04-25T14:53:23.506381Z",
     "shell.execute_reply": "2025-04-25T14:53:23.505626Z",
     "shell.execute_reply.started": "2025-04-25T14:53:18.823463Z"
    }
   },
   "source": [
    "# Improved UNet pipepline with larger dataset - Copied Notebook with Modifications\n",
    "\n",
    "#### [<u>Initial version</u>](https://www.kaggle.com/code/egortrushin/gwi-improved-unet-pipepline-with-larger-dataset)\n",
    "\n",
    "- We used UNet model as introduced in [<u>5 depth U net with residual</u>](https://www.kaggle.com/code/adhok93/5-depth-u-net-with-residual) Notebook.\n",
    "- We used part of Full OpenWFI dataset, which was introduced in [<u>OpenFWI InversionNet Train with 670G Datasets</u>](https://www.kaggle.com/code/seshurajup/openfwi-inversionnet-train-with-670g-datasets) Notebook.\n",
    "\n",
    "#### <u>Copied version</u>\n",
    "\n",
    "- We will use openFWI dataset converted from float32 to float16 ([<u>openfwi_float16_1</u>](https://www.kaggle.com/datasets/egortrushin/open-wfi-1), [<u>openfwi_float16_2</u>](https://www.kaggle.com/datasets/egortrushin/open-wfi-2), and [<u>openfwi_float16_test</u>](https://www.kaggle.com/datasets/egortrushin/open-wfi-test)). Since reading of data from disk is a bottleneck here, the conversion allows us to read data from disk twice faster and to use twice more data in training for the same runtime.\n",
    "\n",
    "#### <u>Added Processing version</u>\n",
    "\n",
    "- I have added denoising and normalization to see if "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T16:28:02.356186Z",
     "iopub.status.busy": "2025-05-14T16:28:02.355913Z",
     "iopub.status.idle": "2025-05-14T16:28:02.366251Z",
     "shell.execute_reply": "2025-05-14T16:28:02.365259Z",
     "shell.execute_reply.started": "2025-05-14T16:28:02.356159Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile config.yaml\n",
    "\n",
    "data_path: /input\n",
    "model: \n",
    "    name: UNet\n",
    "    unet_params:\n",
    "        init_features: 32\n",
    "        depth: 5\n",
    "read_weights: null\n",
    "batch_size: 32\n",
    "print_freq: 1000\n",
    "max_epochs: 30\n",
    "es_epochs: 4\n",
    "seed: 42\n",
    "valid_frac: 16\n",
    "train_frac: 2\n",
    "optimizer:\n",
    "    lr: 0.0005\n",
    "    weight_decay: 0.001\n",
    "scheduler:\n",
    "    params:\n",
    "        factor: 0.316227766\n",
    "        patience: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "import gc\n",
    "import shutil\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import scipy.signal as signal\n",
    "from scipy.signal import butter, filtfilt, sosfilt\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.multiprocessing as mp\n",
    "mp.set_sharing_strategy('file_system')\n",
    "\n",
    "# Utilities\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T16:28:02.367961Z",
     "iopub.status.busy": "2025-05-14T16:28:02.367746Z",
     "iopub.status.idle": "2025-05-14T16:28:06.750047Z",
     "shell.execute_reply": "2025-05-14T16:28:06.749262Z",
     "shell.execute_reply.started": "2025-05-14T16:28:02.367942Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "def inputs_files_to_output_files(input_files):\n",
    "    return [\n",
    "        Path(str(f).replace('seis', 'vel').replace('data', 'model'))\n",
    "        for f in input_files\n",
    "    ]\n",
    "\n",
    "def get_train_files(data_path):\n",
    "\n",
    "    all_inputs = [\n",
    "        f\n",
    "        for f in\n",
    "        Path(data_path).rglob('*.npy')\n",
    "        if ('seis' in f.stem) or ('data' in f.stem)\n",
    "    ]\n",
    "\n",
    "    all_outputs = inputs_files_to_output_files(all_inputs)\n",
    "\n",
    "    assert all(f.exists() for f in all_outputs)\n",
    "\n",
    "    return all_inputs, all_outputs\n",
    "\n",
    "class SeismicDataset(Dataset):\n",
    "    def __init__(self, inputs_files, output_files, n_examples_per_file=500):\n",
    "        assert len(inputs_files) == len(output_files)\n",
    "        self.inputs_files = inputs_files\n",
    "        self.output_files = output_files\n",
    "        self.n_examples_per_file = n_examples_per_file\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs_files) * self.n_examples_per_file\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Calculate file offset and sample offset within file\n",
    "        file_idx = idx // self.n_examples_per_file\n",
    "        sample_idx = idx % self.n_examples_per_file\n",
    "\n",
    "        X = np.load(self.inputs_files[file_idx], mmap_mode='r')\n",
    "        y = np.load(self.output_files[file_idx], mmap_mode='r')\n",
    "\n",
    "        try:\n",
    "            return X[sample_idx].copy(), y[sample_idx].copy()\n",
    "        finally:\n",
    "            del X, y\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, test_files):\n",
    "        self.test_files = test_files\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_files)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        test_file = self.test_files[i]\n",
    "\n",
    "        return np.load(test_file), test_file.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T16:28:06.751541Z",
     "iopub.status.busy": "2025-05-14T16:28:06.751295Z",
     "iopub.status.idle": "2025-05-14T16:28:06.773854Z",
     "shell.execute_reply": "2025-05-14T16:28:06.773264Z",
     "shell.execute_reply.started": "2025-05-14T16:28:06.751525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "class ResidualDoubleConv(nn.Module):\n",
    "    \"\"\"(Convolution => [BN] => ReLU) * 2 + Residual Connection\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "\n",
    "        # First convolution layer\n",
    "        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(mid_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Second convolution layer\n",
    "        self.conv2 = nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Shortcut connection to handle potential channel mismatch\n",
    "        if in_channels == out_channels:\n",
    "            self.shortcut = nn.Identity()\n",
    "        else:\n",
    "            # Projection shortcut: 1x1 conv + BN to match output channels\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x  # Store the input for the residual connection\n",
    "\n",
    "        # First conv block\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        # Second conv block (without final ReLU yet)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        # Apply shortcut to the identity path\n",
    "        identity_mapped = self.shortcut(identity)\n",
    "\n",
    "        # Add the residual connection\n",
    "        out += identity_mapped\n",
    "\n",
    "        # Apply final ReLU\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then ResidualDoubleConv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)\n",
    "            # Input to ResidualDoubleConv = channels from upsampled layer below + channels from skip connection\n",
    "            # Output of ResidualDoubleConv = desired output channels for this decoder stage\n",
    "            self.conv = ResidualDoubleConv(in_channels + out_channels, out_channels) # Use ResidualDoubleConv\n",
    "\n",
    "        else: # Using ConvTranspose2d\n",
    "            # ConvTranspose halves the channels: in_channels -> in_channels // 2\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            # Input channels to ResidualDoubleConv\n",
    "            conv_in_channels = in_channels // 2 # Channels after ConvTranspose\n",
    "            skip_channels = out_channels       # Channels from skip connection\n",
    "            total_in_channels = conv_in_channels + skip_channels\n",
    "            self.conv = ResidualDoubleConv(total_in_channels, out_channels) # Use ResidualDoubleConv\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # x1 is the feature map from the layer below (needs upsampling)\n",
    "        # x2 is the skip connection from the corresponding encoder layer\n",
    "        x1 = self.up(x1)\n",
    "\n",
    "        # Pad x1 if its dimensions don't match x2 after upsampling\n",
    "        # Input is CHW\n",
    "        diffY = x2.size(2) - x1.size(2)\n",
    "        diffX = x2.size(3) - x1.size(3)\n",
    "\n",
    "        # Pad format: (padding_left, padding_right, padding_top, padding_bottom)\n",
    "        x1 = F.pad(\n",
    "            x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2]\n",
    "        )\n",
    "\n",
    "        # Concatenate along the channel dimension\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    \"\"\"1x1 Convolution for the output layer\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"U-Net architecture implementation with Residual Blocks\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_channels=5,\n",
    "        n_classes=1,\n",
    "        init_features=32,\n",
    "        depth=5, # number of pooling layers\n",
    "        bilinear=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "        self.depth = depth\n",
    "\n",
    "        self.initial_pool = nn.AvgPool2d(kernel_size=(14, 1), stride=(14, 1))\n",
    "\n",
    "        # --- Encoder ---\n",
    "        self.encoder_convs = nn.ModuleList() # Store conv blocks\n",
    "        self.encoder_pools = nn.ModuleList() # Store pool layers\n",
    "\n",
    "        # Initial conv block (no pooling before it)\n",
    "        # Use ResidualDoubleConv for the initial convolution block\n",
    "        self.inc = ResidualDoubleConv(n_channels, init_features)\n",
    "        self.encoder_convs.append(self.inc)\n",
    "\n",
    "        current_features = init_features\n",
    "        for _ in range(depth):\n",
    "            # Define convolution block for this stage\n",
    "            conv = ResidualDoubleConv(current_features, current_features * 2)\n",
    "            # Define pooling layer for this stage\n",
    "            pool = nn.MaxPool2d(2)\n",
    "            self.encoder_convs.append(conv)\n",
    "            self.encoder_pools.append(pool)\n",
    "            current_features *= 2\n",
    "\n",
    "        # --- Bottleneck ---\n",
    "        # Use ResidualDoubleConv for the bottleneck\n",
    "        self.bottleneck = ResidualDoubleConv(current_features, current_features)\n",
    "\n",
    "        # --- Decoder ---\n",
    "        self.decoder_blocks = nn.ModuleList()\n",
    "        # Input features start from bottleneck output features\n",
    "        # Output features at each stage are halved\n",
    "        for _ in range(depth):\n",
    "            # Up block uses ResidualDoubleConv internally and handles channels\n",
    "            up_block = Up(current_features, current_features // 2, bilinear)\n",
    "            self.decoder_blocks.append(up_block)\n",
    "            current_features //= 2 # Halve features for next Up block input\n",
    "\n",
    "        # --- Output Layer ---\n",
    "        # Input features are the output features of the last Up block\n",
    "        self.outc = OutConv(current_features, n_classes)\n",
    "\n",
    "    def _pad_or_crop(self, x, target_h=70, target_w=70):\n",
    "        \"\"\"Pads or crops input tensor x to target height and width.\"\"\"\n",
    "        _, _, h, w = x.shape\n",
    "        # Pad Height if needed\n",
    "        if h < target_h:\n",
    "            pad_top = (target_h - h) // 2\n",
    "            pad_bottom = target_h - h - pad_top\n",
    "            x = F.pad(x, (0, 0, pad_top, pad_bottom))  # Pad height only\n",
    "            h = target_h\n",
    "        # Pad Width if needed\n",
    "        if w < target_w:\n",
    "            pad_left = (target_w - w) // 2\n",
    "            pad_right = target_w - w - pad_left\n",
    "            x = F.pad(x, (pad_left, pad_right, 0, 0))  # Pad width only\n",
    "            w = target_w\n",
    "        # Crop Height if needed\n",
    "        if h > target_h:\n",
    "            crop_top = (h - target_h) // 2\n",
    "            # Use slicing to crop\n",
    "            x = x[:, :, crop_top : crop_top + target_h, :]\n",
    "            h = target_h\n",
    "        # Crop Width if needed\n",
    "        if w > target_w:\n",
    "            crop_left = (w - target_w) // 2\n",
    "            x = x[:, :, :, crop_left : crop_left + target_w]\n",
    "            w = target_w\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial pooling and resizing\n",
    "        x_pooled = self.initial_pool(x)\n",
    "        x_resized = self._pad_or_crop(x_pooled, target_h=70, target_w=70)\n",
    "\n",
    "        # --- Encoder Path ---\n",
    "        skip_connections = []\n",
    "        xi = x_resized\n",
    "\n",
    "        # Apply initial conv (inc)\n",
    "        xi = self.encoder_convs[0](xi)\n",
    "        skip_connections.append(xi) # Store output of inc\n",
    "\n",
    "        # Apply subsequent encoder convs and pools\n",
    "        # self.depth is the number of pooling layers\n",
    "        for i in range(self.depth):\n",
    "            # Apply conv block for this stage\n",
    "            xi = self.encoder_convs[i+1](xi)\n",
    "            # Store skip connection *before* pooling\n",
    "            skip_connections.append(xi)\n",
    "            # Apply pooling layer for this stage\n",
    "            xi = self.encoder_pools[i](xi)\n",
    "\n",
    "        # Apply bottleneck conv\n",
    "        xi = self.bottleneck(xi)\n",
    "\n",
    "        # --- Decoder Path ---\n",
    "        xu = xi # Start with bottleneck output\n",
    "        # Iterate through decoder blocks and corresponding skip connections in reverse\n",
    "        for i, block in enumerate(self.decoder_blocks):\n",
    "            # Determine the correct skip connection index from the end\n",
    "            # Example: depth=5. Skips stored: [inc, enc1, enc2, enc3, enc4] (indices 0-4)\n",
    "            # Decoder 0 (Up(1024, 512)) needs skip 4 (enc4)\n",
    "            # Decoder 1 (Up(512, 256)) needs skip 3 (enc3) ...\n",
    "            # Decoder 4 (Up(64, 32)) needs skip 0 (inc)\n",
    "            skip_index = self.depth - 1 - i\n",
    "            skip = skip_connections[skip_index]\n",
    "            xu = block(xu, skip) # Up block combines xu (from below) and skip\n",
    "\n",
    "        # --- Final Output ---\n",
    "        logits = self.outc(xu)\n",
    "        # Apply scaling and offset specific to the problem's target range\n",
    "        output = logits * 1000.0 + 1500.0\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T16:28:06.775018Z",
     "iopub.status.busy": "2025-05-14T16:28:06.774813Z",
     "iopub.status.idle": "2025-05-14T16:28:06.800170Z",
     "shell.execute_reply": "2025-05-14T16:28:06.799440Z",
     "shell.execute_reply.started": "2025-05-14T16:28:06.775002Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Helper functions \n",
    "def butter_bandpass(lowcut, highcut, fs, order=4):\n",
    "    \"\"\"Design a Butterworth bandpass filter.\"\"\"\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band', output='ba')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=4):\n",
    "    \"\"\"Apply a Butterworth bandpass filter to a 1D signal.\"\"\"\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "def denoise_seismic_with_bandpass(data, lowcut=0.004, highcut=0.3, fs=1.2, order=4):\n",
    "    \"\"\"\n",
    "    Apply Butterworth bandpass filter to seismic data.\n",
    "    Handles 2D, 3D, or 4D data.\n",
    "    Inner tqdm progress bars are disabled to reduce verbosity.\n",
    "    \"\"\"\n",
    "    original_shape = data.shape\n",
    "    \n",
    "    if len(original_shape) == 4: # 4D data (e.g., batch, channels, height, width)\n",
    "        denoised_data = np.zeros_like(data)\n",
    "        # Iterate through the first dimension (samples in batch)\n",
    "        for s_idx in tqdm(range(original_shape[0]), desc=\"Processing samples in batch\", leave=False, disable=True):\n",
    "            for c_idx in range(original_shape[1]): # Channels\n",
    "                denoised_data[s_idx, c_idx] = denoise_seismic_with_bandpass(\n",
    "                    data[s_idx, c_idx], lowcut, highcut, fs, order\n",
    "                )\n",
    "        return denoised_data\n",
    "    \n",
    "    elif len(original_shape) == 3: # 3D data (e.g., batch/channels, height, width)\n",
    "        denoised_data = np.zeros_like(data)\n",
    "        # Iterate through the first dimension (channels or slices)\n",
    "        for c_idx in tqdm(range(original_shape[0]), desc=\"Processing channels/slices\", leave=False, disable=True):\n",
    "            denoised_data[c_idx] = denoise_seismic_with_bandpass(\n",
    "                data[c_idx], lowcut, highcut, fs, order\n",
    "            )\n",
    "        return denoised_data\n",
    "    \n",
    "    elif len(original_shape) == 2: # 2D data (height, width) - filter per trace\n",
    "        denoised_data = np.zeros_like(data)\n",
    "        for i in range(data.shape[1]): # Iterate over traces (width)\n",
    "            trace = data[:, i]\n",
    "            # filtfilt requirement: signal length > 3 * filter order\n",
    "            if len(trace) > order * 3:\n",
    "                 denoised_data[:, i] = butter_bandpass_filter(trace, lowcut, highcut, fs, order)\n",
    "            else:\n",
    "                # To avoid verbose output, this warning is kept as a comment.\n",
    "                # print(f\"Warning: Trace length ({len(trace)}) for trace {i} too short for filter order ({order}). Copying original trace.\")\n",
    "                denoised_data[:, i] = trace\n",
    "        return denoised_data\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported data shape: {original_shape}. Expected 2D, 3D, or 4D.\")\n",
    "\n",
    "# Revised memory-efficient function with reduced print output\n",
    "def denoise_training_dataset_memory_efficient(train_loader, lowcut=0.004, highcut=0.3, fs=1.2, order=4):\n",
    "    denoised_inputs_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    print(\"Starting memory-efficient denoising process...\")\n",
    "    \n",
    "    try:\n",
    "        total_batches = len(train_loader)\n",
    "    except TypeError:\n",
    "        total_batches = None \n",
    "\n",
    "    overall_start_time = time.time()\n",
    "\n",
    "    # This is the main progress bar the user will see, updating per batch\n",
    "    for inputs_batch, targets_batch in tqdm(train_loader, desc=\"Denoising batches\", total=total_batches, unit=\"batch\"):\n",
    "        inputs_batch_np = inputs_batch.numpy() \n",
    "        \n",
    "        denoised_batch = denoise_seismic_with_bandpass(\n",
    "            inputs_batch_np, lowcut=lowcut, highcut=highcut, fs=fs, order=order\n",
    "        )\n",
    "\n",
    "        denoised_inputs_list.append(denoised_batch)\n",
    "        targets_list.append(targets_batch.numpy()) \n",
    "    \n",
    "    overall_elapsed_time = time.time() - overall_start_time\n",
    "    # Adding a newline to ensure this print is on a new line after tqdm finishes\n",
    "    print(f\"\\nAll batches processed in {overall_elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    print(\"Concatenating denoised batches and targets...\")\n",
    "    concatenation_start_time = time.time()\n",
    "    \n",
    "    final_denoised_inputs = np.concatenate(denoised_inputs_list, axis=0)\n",
    "    final_targets = np.concatenate(targets_list, axis=0)\n",
    "    \n",
    "    concatenation_elapsed_time = time.time() - concatenation_start_time\n",
    "    print(f\"Concatenation completed in {concatenation_elapsed_time:.2f} seconds.\")\n",
    "    \n",
    "    return final_denoised_inputs, final_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create a custom dataset that applies denoising on-the-fly\n",
    "class DenoisedSeismicDataset(Dataset):\n",
    "    def __init__(self, inputs_files, output_files, n_examples_per_file=500, \n",
    "                 apply_denoising=True, lowcut=0.004, highcut=0.3, fs=1.2, order=4):\n",
    "        assert len(inputs_files) == len(output_files)\n",
    "        self.inputs_files = inputs_files\n",
    "        self.output_files = output_files\n",
    "        self.n_examples_per_file = n_examples_per_file\n",
    "        self.apply_denoising = apply_denoising\n",
    "        self.lowcut = lowcut\n",
    "        self.highcut = highcut\n",
    "        self.fs = fs\n",
    "        self.order = order\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs_files) * self.n_examples_per_file\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Calculate file offset and sample offset within file\n",
    "        file_idx = idx // self.n_examples_per_file\n",
    "        sample_idx = idx % self.n_examples_per_file\n",
    "\n",
    "        X = np.load(self.inputs_files[file_idx], mmap_mode='r')\n",
    "        y = np.load(self.output_files[file_idx], mmap_mode='r')\n",
    "\n",
    "        try:\n",
    "            input_data = X[sample_idx].copy()\n",
    "            target_data = y[sample_idx].copy()\n",
    "            \n",
    "            # Apply denoising if enabled\n",
    "            if self.apply_denoising:\n",
    "                input_data = denoise_seismic_with_bandpass(\n",
    "                    input_data, \n",
    "                    lowcut=self.lowcut, \n",
    "                    highcut=self.highcut, \n",
    "                    fs=self.fs, \n",
    "                    order=self.order\n",
    "                )\n",
    "                \n",
    "            return input_data, target_data\n",
    "        finally:\n",
    "            del X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check available disk space\n",
    "def check_available_disk_space(required_gb, save_dir):\n",
    "    \"\"\"Check if there's enough disk space available.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    disk_usage = shutil.disk_usage(save_dir)\n",
    "    available_gb = disk_usage.free / (1024**3)\n",
    "    \n",
    "    print(f\"Available disk space: {available_gb:.2f} GB\")\n",
    "    print(f\"Required disk space: {required_gb:.2f} GB\")\n",
    "    \n",
    "    return available_gb >= required_gb\n",
    "\n",
    "# Function to denoise and save the dataset\n",
    "def denoise_and_save_dataset(input_files, output_files, save_dir, \n",
    "                            lowcut=0.004, highcut=0.3, fs=1.2, order=4,\n",
    "                            prefix=\"train\"):\n",
    "    \"\"\"Process all files, apply denoising, and save to disk.\"\"\"\n",
    "    # Create directory for denoised data\n",
    "    denoised_dir = os.path.join(save_dir, \"denoised_data\")\n",
    "    os.makedirs(denoised_dir, exist_ok=True)\n",
    "    \n",
    "    # Estimate required disk space\n",
    "    sample_input = np.load(input_files[0], mmap_mode='r')\n",
    "    sample_size_bytes = sample_input.nbytes\n",
    "    total_files = len(input_files)\n",
    "    estimated_gb = (sample_size_bytes * total_files * 2) / (1024**3)  # x2 for inputs and targets\n",
    "    \n",
    "    # Check for sufficient disk space\n",
    "    if not check_available_disk_space(estimated_gb, save_dir):\n",
    "        raise RuntimeError(f\"Not enough disk space. Need approximately {estimated_gb:.2f} GB\")\n",
    "    \n",
    "    denoised_input_paths = []\n",
    "    target_output_paths = []\n",
    "    \n",
    "    print(f\"Processing {len(input_files)} files with prefix '{prefix}'...\")\n",
    "    \n",
    "    # Process each file\n",
    "    for i, (input_file, output_file) in enumerate(tqdm(zip(input_files, output_files), \n",
    "                                                      total=len(input_files),\n",
    "                                                      desc=f\"Denoising {prefix} files\")):\n",
    "        # Generate filenames\n",
    "        base_name = os.path.basename(input_file)\n",
    "        denoised_filename = f\"{prefix}_denoised_{i}_{base_name}\"\n",
    "        target_filename = f\"{prefix}_target_{i}_{os.path.basename(output_file)}\"\n",
    "        \n",
    "        denoised_path = os.path.join(denoised_dir, denoised_filename)\n",
    "        target_path = os.path.join(denoised_dir, target_filename)\n",
    "        \n",
    "        # Add to path lists\n",
    "        denoised_input_paths.append(denoised_path)\n",
    "        target_output_paths.append(target_path)\n",
    "        \n",
    "        # Skip if files already exist\n",
    "        if os.path.exists(denoised_path) and os.path.exists(target_path):\n",
    "            print(f\"Files already exist for {base_name}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Load data\n",
    "            input_data = np.load(input_file)\n",
    "            target_data = np.load(output_file)\n",
    "            \n",
    "            # Apply denoising\n",
    "            denoised_input = denoise_seismic_with_bandpass(\n",
    "                input_data, \n",
    "                lowcut=lowcut, \n",
    "                highcut=highcut, \n",
    "                fs=fs, \n",
    "                order=order\n",
    "            )\n",
    "            \n",
    "            # Save processed data\n",
    "            np.save(denoised_path, denoised_input)\n",
    "            np.save(target_path, target_data)\n",
    "            \n",
    "            # Free memory\n",
    "            del input_data, denoised_input, target_data\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {input_file}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return denoised_input_paths, target_output_paths\n",
    "\n",
    "# Dataset class for pre-denoised data\n",
    "class PreDenoisedSeismicDataset(Dataset):\n",
    "    def __init__(self, inputs_files, output_files, n_examples_per_file=250):\n",
    "        assert len(inputs_files) == len(output_files)\n",
    "        self.inputs_files = inputs_files\n",
    "        self.output_files = output_files\n",
    "        self.n_examples_per_file = n_examples_per_file\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs_files) * self.n_examples_per_file\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Calculate which file and sample within file\n",
    "        file_idx = idx // self.n_examples_per_file\n",
    "        sample_idx = idx % self.n_examples_per_file\n",
    "\n",
    "        try:\n",
    "            # Use memory-mapped loading for efficiency\n",
    "            X = np.load(self.inputs_files[file_idx], mmap_mode='r')\n",
    "            y = np.load(self.output_files[file_idx], mmap_mode='r')\n",
    "            \n",
    "            # Copy to avoid memory issues with mmap\n",
    "            input_data = X[sample_idx].copy()\n",
    "            target_data = y[sample_idx].copy()\n",
    "                \n",
    "            return torch.from_numpy(input_data).float(), torch.from_numpy(target_data).float()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample {idx} from file {self.inputs_files[file_idx]}: {str(e)}\")\n",
    "            # Return zeros with appropriate shape (adjust based on your data dimensions)\n",
    "            return torch.zeros((1, 224, 224), dtype=torch.float32), torch.zeros((1, 224, 224), dtype=torch.float32)\n",
    "        finally:\n",
    "            # Clean up memory-mapped files\n",
    "            if 'X' in locals():\n",
    "                del X\n",
    "            if 'y' in locals():\n",
    "                del y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "def format_time(elapsed):\n",
    "    \"\"\"Take a time in seconds and return a string hh:mm:ss.\"\"\"\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def seed_everything(\n",
    "    seed_value: int\n",
    ") -> None:\n",
    "\n",
    "    random.seed(seed_value) # Python\n",
    "    np.random.seed(seed_value) # cpu vars\n",
    "    torch.manual_seed(seed_value) # cpu  vars    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
    "    if torch.backends.cudnn.is_available:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "_, total = torch.cuda.mem_get_info(device=0)\n",
    "print(f\"GPU memory: {total / 1024**3:.2f}GB\")\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as file_obj:\n",
    "    config = yaml.safe_load(file_obj)\n",
    "print()\n",
    "print(config)\n",
    "if config[\"data_path\"] is None:\n",
    "    config[\"data_path\"] = os.environ[\"TMPDIR\"]\n",
    "    print(\"data_path:\", config[\"data_path\"])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Preparation and training\n",
    "# Refactored training pipeline with pre-denoising\n",
    "# Set random seed\n",
    "seed_everything(config[\"seed\"])\n",
    "\n",
    "# Get data files\n",
    "all_inputs, all_outputs = [], []\n",
    "for x in [\"input/train_samples\", \"input/openfwi_float16_1\", \"input/openfwi_float16_2\"]:\n",
    "    all_inputs1, all_outputs1 = get_train_files(x)\n",
    "    all_inputs.extend(all_inputs1)\n",
    "    all_outputs.extend(all_outputs1)\n",
    "print(\"Total number of input/output files:\", len(all_inputs))\n",
    "\n",
    "# Split into train and validation sets\n",
    "valid_indices = list(range(0, len(all_inputs), config[\"valid_frac\"]))\n",
    "valid_inputs = [all_inputs[i] for i in valid_indices]\n",
    "train_inputs = [f for f in all_inputs if f not in valid_inputs]\n",
    "\n",
    "if config[\"train_frac\"] > 1:\n",
    "    train_indices = list(range(0, len(train_inputs), config[\"train_frac\"]))\n",
    "    train_inputs = [train_inputs[i] for i in train_indices]\n",
    "\n",
    "print(\"Number of train files:\", len(train_inputs))\n",
    "print(\"Number of valid files:\", len(valid_inputs))\n",
    "\n",
    "train_outputs = inputs_files_to_output_files(train_inputs)\n",
    "valid_outputs = inputs_files_to_output_files(valid_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel Adaptive Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactored processing functions with improved efficiency and error handling\n",
    "def histogram_equalization_seismic(data, n_bins=1000, clip_percentile=99.5):\n",
    "    \"\"\"\n",
    "    Apply histogram equalization to seismic data to improve amplitude distribution.\n",
    "    Simplified to work on single samples rather than batches.\n",
    "    \"\"\"\n",
    "    # Get shape information\n",
    "    if data.ndim == 3:  # Single sample: (channels, time, traces)\n",
    "        n_channels, time_steps, n_traces = data.shape\n",
    "        equalized_data = np.zeros_like(data)\n",
    "        \n",
    "        for c in range(n_channels):\n",
    "            channel_data = data[c]\n",
    "            \n",
    "            # Find non-zero values (signal)\n",
    "            non_zero_mask = np.abs(channel_data) > 1e-6\n",
    "            if np.sum(non_zero_mask) > 100:\n",
    "                # Get data values for histogram computation\n",
    "                data_values = channel_data[non_zero_mask]\n",
    "                \n",
    "                # Clip extreme outliers\n",
    "                clip_value = np.percentile(np.abs(data_values), clip_percentile)\n",
    "                clipped_data = np.clip(channel_data, -clip_value, clip_value)\n",
    "                \n",
    "                # Create histogram and cumulative distribution\n",
    "                hist, bin_edges = np.histogram(clipped_data, bins=n_bins, range=(-clip_value, clip_value))\n",
    "                cdf = hist.cumsum() / (hist.sum() + 1e-10)\n",
    "                \n",
    "                # Create mapping function\n",
    "                bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "                equalized = np.interp(clipped_data.flatten(), bin_centers, 2 * cdf - 1).reshape(clipped_data.shape)\n",
    "                \n",
    "                # Apply mild smoothing\n",
    "                equalized = ndimage.gaussian_filter(equalized, sigma=0.5)\n",
    "                equalized_data[c] = np.clip(equalized, -1, 1)\n",
    "            else:\n",
    "                equalized_data[c] = data[c]\n",
    "                \n",
    "        return equalized_data\n",
    "    else:\n",
    "        raise ValueError(f\"Expected 3D array, got {data.ndim}D\")\n",
    "\n",
    "def adaptive_agc_single(data, window_sizes=[50, 100, 200], blend_weights=[0.2, 0.3, 0.5]):\n",
    "    \"\"\"\n",
    "    Apply multi-scale AGC to a single sample.\n",
    "    \"\"\"\n",
    "    if data.ndim == 3:  # (channels, time, traces)\n",
    "        n_channels, time_steps, n_traces = data.shape\n",
    "        balanced_data = np.zeros_like(data)\n",
    "        \n",
    "        # Normalize weights\n",
    "        blend_weights = np.array(blend_weights) / np.sum(blend_weights)\n",
    "        \n",
    "        for c in range(n_channels):\n",
    "            for t in range(n_traces):\n",
    "                trace = data[c, :, t]\n",
    "                \n",
    "                # Skip if trace is mostly zeros\n",
    "                if np.sum(np.abs(trace)) < 1e-6:\n",
    "                    balanced_data[c, :, t] = trace\n",
    "                    continue\n",
    "                \n",
    "                blended_trace = np.zeros_like(trace)\n",
    "                \n",
    "                # Apply AGC at multiple scales\n",
    "                for i, window_size in enumerate(window_sizes):\n",
    "                    half_win = window_size // 2\n",
    "                    agc_trace = np.zeros_like(trace)\n",
    "                    \n",
    "                    for i_sample in range(time_steps):\n",
    "                        win_start = max(0, i_sample - half_win)\n",
    "                        win_end = min(time_steps, i_sample + half_win + 1)\n",
    "                        \n",
    "                        # Calculate RMS in window\n",
    "                        window_rms = np.sqrt(np.mean(trace[win_start:win_end]**2) + 1e-10)\n",
    "                        agc_trace[i_sample] = trace[i_sample] / window_rms\n",
    "                    \n",
    "                    blended_trace += blend_weights[i] * agc_trace\n",
    "                \n",
    "                # Normalize\n",
    "                max_abs = np.max(np.abs(blended_trace))\n",
    "                if max_abs > 1.0:\n",
    "                    blended_trace /= max_abs\n",
    "                    \n",
    "                balanced_data[c, :, t] = blended_trace\n",
    "        \n",
    "        # Apply final smoothing\n",
    "        for c in range(n_channels):\n",
    "            balanced_data[c] = ndimage.gaussian_filter(balanced_data[c], sigma=0.5)\n",
    "            \n",
    "        return np.clip(balanced_data, -1.0, 1.0)\n",
    "    else:\n",
    "        raise ValueError(f\"Expected 3D array, got {data.ndim}D\")\n",
    "\n",
    "def amplitude_distribution_matching_single(data, alpha=0.4, smoothing_sigma=0.5):\n",
    "    \"\"\"\n",
    "    Apply amplitude distribution matching to a single sample.\n",
    "    \"\"\"\n",
    "    if data.ndim == 3:  # (channels, time, traces)\n",
    "        n_channels, time_steps, n_traces = data.shape\n",
    "        matched_data = np.copy(data)\n",
    "        \n",
    "        for c in range(n_channels):\n",
    "            channel_data = data[c].flatten()\n",
    "            \n",
    "            # Find non-zero values\n",
    "            non_zero_mask = np.abs(channel_data) > 0.01\n",
    "            if np.sum(non_zero_mask) > 100:\n",
    "                channel_values = channel_data[non_zero_mask]\n",
    "                \n",
    "                # Calculate contrast enhancement\n",
    "                enhancement = np.sign(channel_values) * (np.abs(channel_values) ** alpha)\n",
    "                \n",
    "                # Create mapping\n",
    "                sorted_orig = np.sort(channel_values)\n",
    "                sorted_enhanced = np.sort(enhancement)\n",
    "                \n",
    "                # Apply mapping to each trace\n",
    "                for t in range(n_traces):\n",
    "                    trace = matched_data[c, :, t]\n",
    "                    mapped_trace = np.interp(trace, sorted_orig, sorted_enhanced)\n",
    "                    \n",
    "                    if smoothing_sigma > 0:\n",
    "                        mapped_trace = ndimage.gaussian_filter1d(mapped_trace, sigma=smoothing_sigma)\n",
    "                    \n",
    "                    matched_data[c, :, t] = mapped_trace\n",
    "        \n",
    "        # Normalize\n",
    "        for c in range(n_channels):\n",
    "            max_abs = np.max(np.abs(matched_data[c]))\n",
    "            if max_abs > 1.0:\n",
    "                matched_data[c] /= max_abs\n",
    "                \n",
    "        return np.clip(matched_data, -1.0, 1.0)\n",
    "    else:\n",
    "        raise ValueError(f\"Expected 3D array, got {data.ndim}D\")\n",
    "\n",
    "def apply_channel_adaptive_processing(data):\n",
    "    \"\"\"\n",
    "    Apply different processing strategies for different channels.\n",
    "    \"\"\"\n",
    "    if data.ndim != 3:\n",
    "        raise ValueError(f\"Expected 3D array (channels, time, traces), got {data.ndim}D\")\n",
    "        \n",
    "    n_channels = data.shape[0]\n",
    "    processed_data = np.copy(data)\n",
    "    \n",
    "    # Define channel groups\n",
    "    linear_channels = [0, 4]\n",
    "    reflection_channels = [1, 2, 3]\n",
    "    \n",
    "    # Process linear channels with AGC\n",
    "    for c in linear_channels:\n",
    "        if c < n_channels:\n",
    "            channel_data = processed_data[c:c+1]\n",
    "            processed_channel = adaptive_agc_single(\n",
    "                channel_data,\n",
    "                window_sizes=[30, 100, 200],\n",
    "                blend_weights=[0.3, 0.4, 0.3]\n",
    "            )\n",
    "            processed_data[c] = processed_channel[0]\n",
    "    \n",
    "    # Process reflection channels with distribution matching\n",
    "    for c in reflection_channels:\n",
    "        if c < n_channels:\n",
    "            channel_data = processed_data[c:c+1]\n",
    "            processed_channel = amplitude_distribution_matching_single(\n",
    "                channel_data,\n",
    "                alpha=0.4,\n",
    "                smoothing_sigma=0.5\n",
    "            )\n",
    "            processed_data[c] = processed_channel[0]\n",
    "    \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this new function for batch processing with progress tracking\n",
    "def preprocess_dataset_with_progress(inputs_files, output_files, processing_type='adaptive', \n",
    "                                    n_examples_per_file=500, save_preprocessed=False, save_dir=None):\n",
    "    \"\"\"\n",
    "    Preprocess entire dataset with overall progress tracking.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    inputs_files : list\n",
    "        List of input file paths\n",
    "    output_files : list\n",
    "        List of output file paths\n",
    "    processing_type : str\n",
    "        Type of processing: 'adaptive', 'histogram', 'agc', 'distribution'\n",
    "    n_examples_per_file : int\n",
    "        Number of examples per file\n",
    "    save_preprocessed : bool\n",
    "        Whether to save preprocessed data to disk\n",
    "    save_dir : str\n",
    "        Directory to save preprocessed data (if save_preprocessed=True)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (processed_inputs_list, targets_list) or (processed_input_paths, target_paths) if saved\n",
    "    \"\"\"\n",
    "    total_samples = len(inputs_files) * n_examples_per_file\n",
    "    processed_inputs_list = []\n",
    "    targets_list = []\n",
    "    processed_input_paths = []\n",
    "    target_paths = []\n",
    "    \n",
    "    print(f\"Starting {processing_type} processing for {total_samples} samples...\")\n",
    "    \n",
    "    # Create overall progress bar\n",
    "    with tqdm(total=total_samples, desc=f\"Applying {processing_type} processing\") as pbar:\n",
    "        \n",
    "        for file_idx, (input_file, output_file) in enumerate(zip(inputs_files, output_files)):\n",
    "            # Load the entire file\n",
    "            X = np.load(input_file)\n",
    "            y = np.load(output_file)\n",
    "            \n",
    "            if save_preprocessed and save_dir:\n",
    "                # Create arrays to hold processed data for this file\n",
    "                processed_X = np.zeros_like(X)\n",
    "                \n",
    "            for sample_idx in range(min(n_examples_per_file, len(X))):\n",
    "                input_data = X[sample_idx].copy()\n",
    "                target_data = y[sample_idx].copy()\n",
    "                \n",
    "                # Normalize the input data\n",
    "                input_mean = np.mean(input_data)\n",
    "                input_std = np.std(input_data) + 1e-8\n",
    "                normalized_input = (input_data - input_mean) / input_std\n",
    "                \n",
    "                # Apply selected processing\n",
    "                if processing_type == 'adaptive':\n",
    "                    processed_input = apply_channel_adaptive_processing(normalized_input)\n",
    "                elif processing_type == 'histogram':\n",
    "                    processed_input = histogram_equalization_seismic(normalized_input)\n",
    "                elif processing_type == 'agc':\n",
    "                    processed_input = adaptive_agc_single(normalized_input)\n",
    "                elif processing_type == 'distribution':\n",
    "                    processed_input = amplitude_distribution_matching_single(normalized_input)\n",
    "                else:\n",
    "                    processed_input = normalized_input\n",
    "                \n",
    "                if save_preprocessed and save_dir:\n",
    "                    processed_X[sample_idx] = processed_input\n",
    "                else:\n",
    "                    processed_inputs_list.append(processed_input)\n",
    "                    targets_list.append(target_data)\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "            \n",
    "            # Save processed file if requested\n",
    "            if save_preprocessed and save_dir:\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                base_name = os.path.basename(input_file)\n",
    "                processed_filename = f\"processed_{processing_type}_{base_name}\"\n",
    "                target_filename = f\"target_{os.path.basename(output_file)}\"\n",
    "                \n",
    "                processed_path = os.path.join(save_dir, processed_filename)\n",
    "                target_path = os.path.join(save_dir, target_filename)\n",
    "                \n",
    "                np.save(processed_path, processed_X[:n_examples_per_file])\n",
    "                np.save(target_path, y[:n_examples_per_file])\n",
    "                \n",
    "                processed_input_paths.append(processed_path)\n",
    "                target_paths.append(target_path)\n",
    "                \n",
    "            # Free memory\n",
    "            del X, y\n",
    "            if save_preprocessed and save_dir:\n",
    "                del processed_X\n",
    "            gc.collect()\n",
    "    \n",
    "    print(f\"Processing completed!\")\n",
    "    \n",
    "    if save_preprocessed and save_dir:\n",
    "        return processed_input_paths, target_paths\n",
    "    else:\n",
    "        return processed_inputs_list, targets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset that applies channel adaptive processing on-the-fly\n",
    "# Modified dataset class with optional preprocessing\n",
    "class ChannelAdaptiveSeismicDataset(Dataset):\n",
    "    def __init__(self, inputs_files, output_files, n_examples_per_file=500, \n",
    "                 apply_processing=True, processing_type='adaptive', \n",
    "                 preprocess_all=False, cache_processed=False):\n",
    "        \"\"\"\n",
    "        Dataset with channel adaptive processing.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        processing_type : str\n",
    "            Type of processing: 'adaptive', 'histogram', 'agc', 'distribution', or None\n",
    "        preprocess_all : bool\n",
    "            If True, preprocess all data at initialization with progress bar\n",
    "        cache_processed : bool\n",
    "            If True and preprocess_all=True, cache processed data in memory\n",
    "        \"\"\"\n",
    "        assert len(inputs_files) == len(output_files)\n",
    "        self.inputs_files = inputs_files\n",
    "        self.output_files = output_files\n",
    "        self.n_examples_per_file = n_examples_per_file\n",
    "        self.apply_processing = apply_processing\n",
    "        self.processing_type = processing_type\n",
    "        self.cache_processed = cache_processed\n",
    "        self.processed_cache = {}\n",
    "        \n",
    "        # Preprocess all data if requested\n",
    "        if preprocess_all and apply_processing and processing_type:\n",
    "            self._preprocess_all_data()\n",
    "\n",
    "    def _preprocess_all_data(self):\n",
    "        \"\"\"Preprocess all data with progress tracking.\"\"\"\n",
    "        total_samples = len(self.inputs_files) * self.n_examples_per_file\n",
    "        \n",
    "        print(f\"Preprocessing {total_samples} samples with {self.processing_type} processing...\")\n",
    "        \n",
    "        with tqdm(total=total_samples, desc=f\"Preprocessing dataset\") as pbar:\n",
    "            for file_idx, (input_file, output_file) in enumerate(zip(self.inputs_files, self.output_files)):\n",
    "                X = np.load(input_file)\n",
    "                y = np.load(output_file)\n",
    "                \n",
    "                for sample_idx in range(min(self.n_examples_per_file, len(X))):\n",
    "                    global_idx = file_idx * self.n_examples_per_file + sample_idx\n",
    "                    \n",
    "                    input_data = X[sample_idx].copy()\n",
    "                    target_data = y[sample_idx].copy()\n",
    "                    \n",
    "                    # Process the data\n",
    "                    processed_input = self._process_sample(input_data)\n",
    "                    \n",
    "                    if self.cache_processed:\n",
    "                        self.processed_cache[global_idx] = (processed_input, target_data)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "                \n",
    "                del X, y\n",
    "                gc.collect()\n",
    "        \n",
    "        print(\"Preprocessing completed!\")\n",
    "\n",
    "    def _process_sample(self, input_data):\n",
    "        \"\"\"Process a single sample.\"\"\"\n",
    "        # Normalize the input data\n",
    "        input_mean = np.mean(input_data)\n",
    "        input_std = np.std(input_data) + 1e-8\n",
    "        normalized_input = (input_data - input_mean) / input_std\n",
    "        \n",
    "        # Apply selected processing\n",
    "        if self.processing_type == 'adaptive':\n",
    "            processed_input = apply_channel_adaptive_processing(normalized_input)\n",
    "        elif self.processing_type == 'histogram':\n",
    "            processed_input = histogram_equalization_seismic(normalized_input)\n",
    "        elif self.processing_type == 'agc':\n",
    "            processed_input = adaptive_agc_single(normalized_input)\n",
    "        elif self.processing_type == 'distribution':\n",
    "            processed_input = amplitude_distribution_matching_single(normalized_input)\n",
    "        else:\n",
    "            processed_input = normalized_input\n",
    "            \n",
    "        return processed_input\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs_files) * self.n_examples_per_file\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Check cache first\n",
    "        if idx in self.processed_cache:\n",
    "            return self.processed_cache[idx]\n",
    "        \n",
    "        # Calculate file offset and sample offset within file\n",
    "        file_idx = idx // self.n_examples_per_file\n",
    "        sample_idx = idx % self.n_examples_per_file\n",
    "\n",
    "        X = np.load(self.inputs_files[file_idx], mmap_mode='r')\n",
    "        y = np.load(self.output_files[file_idx], mmap_mode='r')\n",
    "\n",
    "        try:\n",
    "            input_data = X[sample_idx].copy()\n",
    "            target_data = y[sample_idx].copy()\n",
    "            \n",
    "            # Apply processing if enabled and not already processed\n",
    "            if self.apply_processing and self.processing_type and idx not in self.processed_cache:\n",
    "                input_data = self._process_sample(input_data)\n",
    "                \n",
    "            return input_data, target_data\n",
    "        finally:\n",
    "            del X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoise Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for denoising\n",
    "denoise_params = {\n",
    "    \"lowcut\": 0.002,\n",
    "    \"highcut\": 0.3,\n",
    "    \"fs\": 1.2,\n",
    "    \"order\": 4\n",
    "}\n",
    "\n",
    "# Create directory for saving denoised data\n",
    "save_dir = config.get(\"denoised_save_dir\", \"denoised_data\")\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and save denoised training data\n",
    "#print(\"\\nPreprocessing and saving denoised training data...\")\n",
    "#denoised_train_inputs, denoised_train_outputs = denoise_and_save_dataset(\n",
    "#    train_inputs, \n",
    "#    train_outputs, \n",
    "#    save_dir=save_dir,\n",
    "#    prefix=\"train\", \n",
    "#    **denoise_params\n",
    "#)\n",
    "\n",
    "# Process and save denoised validation data\n",
    "#print(\"\\nPreprocessing and saving denoised validation data...\")\n",
    "#denoised_valid_inputs, denoised_valid_outputs = denoise_and_save_dataset(\n",
    "#    valid_inputs, \n",
    "#    valid_outputs, \n",
    "#    save_dir=save_dir,\n",
    "#    prefix=\"valid\", \n",
    "#    **denoise_params\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and save to disk\n",
    "processed_save_dir = \"preprocessed_adaptive\"\n",
    "\n",
    "print(\"Preprocessing training data...\")\n",
    "processed_train_inputs, processed_train_outputs = preprocess_dataset_with_progress(\n",
    "    train_inputs, \n",
    "    train_outputs,\n",
    "    processing_type='adaptive',\n",
    "    n_examples_per_file=500,\n",
    "    save_preprocessed=True,\n",
    "    save_dir=os.path.join(processed_save_dir, \"train\")\n",
    ")\n",
    "\n",
    "print(\"\\nPreprocessing validation data...\")\n",
    "processed_valid_inputs, processed_valid_outputs = preprocess_dataset_with_progress(\n",
    "    valid_inputs, \n",
    "    valid_outputs,\n",
    "    processing_type='adaptive',\n",
    "    n_examples_per_file=500,\n",
    "    save_preprocessed=True,\n",
    "    save_dir=os.path.join(processed_save_dir, \"valid\")\n",
    ")\n",
    "\n",
    "# Then use regular SeismicDataset with preprocessed files\n",
    "dstrain = SeismicDataset(processed_train_inputs, processed_train_outputs)\n",
    "dsvalid = SeismicDataset(processed_valid_inputs, processed_valid_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-14T18:20:26.578Z",
     "iopub.execute_input": "2025-05-14T16:28:53.018309Z",
     "iopub.status.busy": "2025-05-14T16:28:53.017978Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create datasets using pre-denoised data\n",
    "#dstrain = PreDenoisedSeismicDataset(\n",
    "#    denoised_train_inputs, \n",
    "#    denoised_train_outputs,\n",
    "#    n_examples_per_file=config.get(\"n_examples_per_file\", 500)\n",
    "#)\n",
    "\n",
    "#dsvalid = PreDenoisedSeismicDataset(\n",
    "#    denoised_valid_inputs, \n",
    "#    denoised_valid_outputs,\n",
    "#    n_examples_per_file=config.get(\"n_examples_per_file\", 500)\n",
    "#)\n",
    "\n",
    "# Load seismic\n",
    "#dstrain = SeismicDataset(train_inputs, train_outputs)\n",
    "#dsvalid = SeismicDataset(valid_inputs, valid_outputs)\n",
    "\n",
    "# Create datasets with channel adaptive processing for training\n",
    "dstrain = ChannelAdaptiveSeismicDataset(\n",
    "    train_inputs, \n",
    "    train_outputs, \n",
    "    apply_processing=True,\n",
    "    processing_type='adaptive'  # Options: 'adaptive', 'histogram', 'agc', 'distribution', None\n",
    ")\n",
    "\n",
    "# For validation, you can choose whether to apply processing\n",
    "dsvalid = ChannelAdaptiveSeismicDataset(\n",
    "    valid_inputs, \n",
    "    valid_outputs, \n",
    "    apply_processing=False  # Usually don't process validation data\n",
    ")\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "dltrain = DataLoader(\n",
    "    dstrain,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=True,  # Can shuffle now that we're not processing on-the-fly\n",
    "    pin_memory=config.get(\"pin_memory\", False),\n",
    "    drop_last=True,\n",
    "    num_workers=config.get(\"num_workers\", 0),\n",
    "    persistent_workers=config.get(\"persistent_workers\", False),\n",
    ")\n",
    "\n",
    "dlvalid = DataLoader(\n",
    "    dsvalid,\n",
    "    batch_size=4*config[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    pin_memory=config.get(\"pin_memory\", False),\n",
    "    drop_last=False,\n",
    "    num_workers=config.get(\"num_workers\", 0),\n",
    "    persistent_workers=config.get(\"persistent_workers\", False),\n",
    ")\n",
    "\n",
    "# Set up device, model, optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(**config[\"model\"][\"unet_params\"]).to(device)\n",
    "\n",
    "if config[\"read_weights\"] is not None:\n",
    "    print(\"Reading weights from:\", config[\"read_weights\"])\n",
    "    model.load_state_dict(torch.load(config[\"read_weights\"], weights_only=True))\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), **config[\"optimizer\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', **config[\"scheduler\"][\"params\"])\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = 10000.0\n",
    "epochs_wo_improvement = 0\n",
    "t0 = time.time()\n",
    "\n",
    "for epoch in range(1, config[\"max_epochs\"] + 1):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    \n",
    "    for step, (inputs, targets) in enumerate(dltrain):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.autocast(device_type=\"cuda\"):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        if step % config[\"print_freq\"] == config[\"print_freq\"] - 1 or step == len(dltrain) - 1:\n",
    "            trn_loss = np.mean(train_losses)\n",
    "            t1 = format_time(time.time() - t0)\n",
    "            free, total = torch.cuda.mem_get_info(device=0)\n",
    "            mem_used = (total - free) / 1024**3\n",
    "            lr = optimizer.param_groups[-1]['lr']\n",
    "            print(\n",
    "                f\"Epoch: {epoch:02d}  Step {step+1}/{len(dltrain)}  Trn Loss: {trn_loss:.2f}  LR: {lr:.2e}  GPU Usage: {mem_used:.2f}GB  Elapsed Time: {t1}\",\n",
    "                flush=True,\n",
    "            )\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    valid_losses = []\n",
    "    \n",
    "    for inputs, targets in dlvalid:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            with torch.autocast(device_type=\"cuda\"):\n",
    "                outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        valid_losses.append(loss.item())\n",
    "\n",
    "    t1 = format_time(time.time() - t0)\n",
    "    trn_loss = np.mean(train_losses)\n",
    "    val_loss = np.mean(valid_losses)\n",
    "\n",
    "    free, total = torch.cuda.mem_get_info(device=0)\n",
    "    mem_used = (total - free) / 1024**3\n",
    "\n",
    "    print(\n",
    "        f\"\\nEpoch: {epoch:02d}  Trn Loss: {trn_loss:.2f}  Val Loss: {val_loss:.2f}  GPU Usage: {mem_used:.2f}GB  Elapsed Time: {t1}\",\n",
    "        flush=True,\n",
    "    )\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_wo_improvement = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        print(f\"\\nNew best val_loss: {val_loss:.2f}\\n\", flush=True)\n",
    "    else:\n",
    "        epochs_wo_improvement += 1\n",
    "        print(f\"\\nEpochs without improvement: {epochs_wo_improvement}\\n\", flush=True)\n",
    "\n",
    "    if epochs_wo_improvement == config[\"es_epochs\"]:\n",
    "        break\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL TRAINING COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Print model summary\n",
    "print(f\"\\nModel Summary:\")\n",
    "print(f\"Architecture: UNet\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Total training time: {format_time(time.time() - t0)}\")\n",
    "\n",
    "# Print model structure\n",
    "print(\"\\nModel Structure:\")\n",
    "print(model)\n",
    "print(\"=\"*50)\n",
    "\n",
    "torch.save(model.state_dict(), f\"{best_val_loss:.4f}_model.pth\")\n",
    "\n",
    "# Create a summary text file\n",
    "summary_filename = f\"model_summary_{best_val_loss:.4f}.txt\"\n",
    "\n",
    "with open(summary_filename, 'w') as f:\n",
    "    # Write header\n",
    "    f.write(\"=\"*50 + \"\\n\")\n",
    "    f.write(\"MODEL TRAINING SUMMARY\\n\")\n",
    "    f.write(\"=\"*50 + \"\\n\\n\")\n",
    "    \n",
    "    # Write model information\n",
    "    f.write(\"MODEL INFORMATION\\n\")\n",
    "    f.write(f\"Architecture: UNet\\n\")\n",
    "    f.write(f\"Total parameters: {total_params:,}\\n\")\n",
    "    f.write(f\"Trainable parameters: {trainable_params:,}\\n\")\n",
    "    f.write(f\"Best validation loss: {best_val_loss:.4f}\\n\")\n",
    "    f.write(f\"Total training time: {format_time(time.time() - t0)}\\n\\n\")\n",
    "    \n",
    "    # Write configuration information\n",
    "    f.write(\"MODEL CONFIGURATION\\n\")\n",
    "    for key, value in config.items():\n",
    "        if isinstance(value, dict):\n",
    "            f.write(f\"{key}:\\n\")\n",
    "            for sub_key, sub_value in value.items():\n",
    "                f.write(f\"  {sub_key}: {sub_value}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    # Write model structure\n",
    "    f.write(\"MODEL STRUCTURE\\n\")\n",
    "    f.write(str(model) + \"\\n\")\n",
    "    \n",
    "    f.write(\"=\"*50 + \"\\n\")\n",
    "\n",
    "print(f\"\\nModel summary saved to {summary_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Print model summary\n",
    "print(f\"\\nModel Summary:\")\n",
    "print(f\"Architecture: UNet\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Total training time: {format_time(time.time() - t0)}\")\n",
    "\n",
    "# Print model structure\n",
    "print(\"\\nModel Structure:\")\n",
    "print(model)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-14T16:28:07.669373Z",
     "iopub.status.idle": "2025-05-14T16:28:07.669589Z",
     "shell.execute_reply": "2025-05-14T16:28:07.669495Z",
     "shell.execute_reply.started": "2025-05-14T16:28:07.669486Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"==================== Inference ===============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-14T16:28:07.670246Z",
     "iopub.status.idle": "2025-05-14T16:28:07.670565Z",
     "shell.execute_reply": "2025-05-14T16:28:07.670421Z",
     "shell.execute_reply.started": "2025-05-14T16:28:07.670405Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "t0 = time.time()\n",
    "\n",
    "test_files = list(Path(\"input/openfwi_float16_test/test\").glob(\"*.npy\"))\n",
    "x_cols = [f\"x_{i}\" for i in range(1, 70, 2)]\n",
    "fieldnames = [\"oid_ypos\"] + x_cols\n",
    "ds = TestDataset(test_files)\n",
    "dl = DataLoader(ds, batch_size=4*config[\"batch_size\"], num_workers=0, pin_memory=False)\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_model.pth\", weights_only=True))\n",
    "\n",
    "model.eval()\n",
    "with open(\"submission.csv\", \"wt\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for inputs, oids_test in dl:\n",
    "        inputs = inputs.to(device)\n",
    "        with torch.inference_mode():\n",
    "            with torch.autocast(device_type=\"cuda\"):\n",
    "                outputs = model(inputs)\n",
    "\n",
    "        y_preds = outputs[:, 0].cpu().numpy()\n",
    "\n",
    "        for y_pred, oid_test in zip(y_preds, oids_test):\n",
    "            for y_pos in range(70):\n",
    "                row = dict(zip(x_cols, [y_pred[y_pos, x_pos] for x_pos in range(1, 70, 2)]))\n",
    "                row[\"oid_ypos\"] = f\"{oid_test}_y_{y_pos}\"\n",
    "\n",
    "                writer.writerow(row)\n",
    "\n",
    "t1 = format_time(time.time() - t0)\n",
    "print(f\"Inference Time: {t1}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11756775,
     "sourceId": 39763,
     "sourceType": "competition"
    },
    {
     "datasetId": 7253205,
     "sourceId": 11568812,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7253605,
     "sourceId": 11569667,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7253661,
     "sourceId": 11569755,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
